{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36064a28-f020-4d6f-8c5a-3e8ba396b75e",
   "metadata": {},
   "source": [
    "## Loading and checking data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2bc90d-98f3-41c8-b061-e7a23b6a77d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Reading output of AlphaPose\n",
    "with open('3d_pose_extraction/raw_ilya_hannah_dyads.json') as f:\n",
    "    output = json.load(f)\n",
    "print(output[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7f5eb4-9e5b-4763-93b7-94c7ec30f598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the information for every frame\n",
    "poses_per_frame = {}\n",
    "for frame in output:\n",
    "    image_id = int(frame['image_id'].split('.')[0])\n",
    "    \n",
    "    try:\n",
    "        poses_per_frame[image_id] += 1\n",
    "    except:\n",
    "        poses_per_frame[image_id] = 1\n",
    "\n",
    "# Are there frames in which no one is identified?\n",
    "missing_frames = []\n",
    "for k in [*poses_per_frame.keys()][:-1]:\n",
    "    if poses_per_frame.get(k+1) is None:\n",
    "        missing_frames.append(k+1)\n",
    "print('Missing frames: {}\\n'.format(missing_frames))\n",
    "\n",
    "print('Possible number of people per frame: {}\\n'.format(np.unique([*poses_per_frame.values()])))\n",
    "\n",
    "# For some frames, either more than 2 people are identified or only 1 person is identified\n",
    "less_instances = []\n",
    "more_instances = []\n",
    "for k, v in poses_per_frame.items():\n",
    "    if v > 2:\n",
    "        more_instances.append(k)\n",
    "    elif v < 2:\n",
    "        less_instances.append(k)\n",
    "print('Frames with 1 person: {}\\n'.format(less_instances))\n",
    "print('Frames with more than 2 people: {}'.format(more_instances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c168291-7855-4192-b2c2-00dfaacdf300",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Visualizing 3 dancers on a frame\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "axes = []\n",
    "color = ['red', 'blue', 'green']\n",
    "skeleton = [(0, 1), (0, 2), (0, 3), (1, 4), (2, 5), (3, 6), (4, 7), (5, 8), (6, 9), (7, 10),\n",
    "            (8, 11), (9, 12), (9, 13), (9, 14), (12, 15), (13, 16), (14, 17), (16, 18), (17, 19), \n",
    "            (18, 20), (19, 21), (20, 22), (21, 23)]\n",
    "\n",
    "indexes = [6, 7, 8]\n",
    "for i in range(len(indexes)):\n",
    "    subplot_number = 131+i\n",
    "    axes.append(fig.add_subplot(subplot_number, projection=\"3d\"))\n",
    "    axes[i].set_xlim([-1, 1])\n",
    "    axes[i].set_ylim([-1, 1])\n",
    "    axes[i].set_zlim([-1, 1])\n",
    "\n",
    "    scatter_points = np.array(output[indexes[i]]['pred_xyz_jts'])\n",
    "    axes[i].scatter(scatter_points[:, 2], scatter_points[:, 0], -scatter_points[:, 1], color=color[i])\n",
    "\n",
    "    for (start, end) in skeleton:\n",
    "        xs = [scatter_points[start, 2], scatter_points[end, 2]]\n",
    "        ys = [scatter_points[start, 0], scatter_points[end, 0]]\n",
    "        zs = [-scatter_points[start, 1], -scatter_points[end, 1]]\n",
    "        axes[i].plot(xs, ys, zs, color='grey')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950102da-5dfa-4c55-a9dd-23c02fac8b4d",
   "metadata": {},
   "source": [
    "## Handling missing frames, single person frames and 3 people frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900e22dc-4f8d-4f4f-a1a0-432b998135e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing frames with more or less than 2 people\n",
    "filtered_output = output.copy()\n",
    "wrong_instances = sorted(set(more_instances).union(set(less_instances)))\n",
    "\n",
    "for i in wrong_instances:\n",
    "    # Filtering frames with more than 2 people\n",
    "    if poses_per_frame[i] > 2:\n",
    "        scores = []\n",
    "        for j in range(poses_per_frame[i]):\n",
    "            scores.append(filtered_output[i*2+j]['score'])\n",
    "        indices = np.argsort(scores)[:poses_per_frame[i]-2]\n",
    "        \n",
    "        if len(indices) > 1:\n",
    "            indices = -np.sort(-indices)\n",
    "\n",
    "        for z in indices:\n",
    "            filtered_output.pop(i*2+z)\n",
    "\n",
    "    # Enriching frames with only 1 person\n",
    "    elif poses_per_frame[i] < 2:\n",
    "        only_person = np.array(filtered_output[i*2]['pred_xyz_jts'])\n",
    "        to_compare_1 = np.array(filtered_output[i*2-1]['pred_xyz_jts'])\n",
    "        to_compare_2 = np.array(filtered_output[i*2-2]['pred_xyz_jts'])\n",
    "        \n",
    "        distance_1 = np.sum(np.linalg.norm(only_person - to_compare_1))\n",
    "        distance_2 = np.sum(np.linalg.norm(only_person - to_compare_2))\n",
    "\n",
    "        if distance_1 > distance_2:\n",
    "            new_data = filtered_output[i*2-1].copy()\n",
    "            new_data['image_id'] = filtered_output[i*2]['image_id']\n",
    "            filtered_output.insert(i*2+1, new_data)\n",
    "        else:\n",
    "            new_data = filtered_output[i*2-2].copy()\n",
    "            new_data['image_id'] = filtered_output[i*2]['image_id']\n",
    "            filtered_output.insert(i*2, new_data)\n",
    "        \n",
    "filtered_poses_per_frame = {}\n",
    "for frame in filtered_output:\n",
    "    image_id = int(frame['image_id'].split('.')[0])\n",
    "    \n",
    "    try:\n",
    "        filtered_poses_per_frame[image_id] += 1\n",
    "    except:\n",
    "        filtered_poses_per_frame[image_id] = 1\n",
    "print('Possible number of people per frame: {}\\n'.format(np.unique([*filtered_poses_per_frame.values()])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e117bb9-2fe7-4d15-9622-c66de20bb434",
   "metadata": {},
   "source": [
    "## First visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9eec897-db0f-4fd6-86aa-16939d6b2224",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "\n",
    "from matplotlib import rcParams\n",
    "rcParams['animation.embed_limit'] = 100\n",
    "\n",
    "def animation(filtered_output, np_flag=False, interval=100):\n",
    "    fig = plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    ax1 = fig.add_subplot(121, projection=\"3d\")\n",
    "    ax1.set_xlim([-1, 1])\n",
    "    ax1.set_ylim([-1, 1])\n",
    "    ax1.set_zlim([-1, 1])\n",
    "    scatt1 = ax1.scatter([], [], [], color='red')\n",
    "    lines1 = [ax1.plot([], [], [], 'gray')[0] for _ in skeleton]\n",
    "    \n",
    "    ax2 = fig.add_subplot(122, projection=\"3d\")\n",
    "    ax2.set_xlim([-1, 1])\n",
    "    ax2.set_ylim([-1, 1])\n",
    "    ax2.set_zlim([-1, 1])\n",
    "    scatt2 = ax2.scatter([], [], [], color='blue')\n",
    "    lines2 = [ax2.plot([], [], [], 'gray')[0] for _ in skeleton]\n",
    "\n",
    "    if np_flag:\n",
    "        person_1_poses = filtered_output[0::2, :, :]\n",
    "        person_2_poses = filtered_output[1::2, :, :]\n",
    "    \n",
    "    else:\n",
    "        person_1_poses = []\n",
    "        person_2_poses = []\n",
    "        for i, frame in enumerate(filtered_output):\n",
    "            if i%2 == 0:\n",
    "                person_1_poses.append(frame['pred_xyz_jts'])\n",
    "            else:\n",
    "                person_2_poses.append(frame['pred_xyz_jts'])\n",
    "        person_1_poses = np.array(person_1_poses)\n",
    "        person_2_poses = np.array(person_2_poses)\n",
    "    \n",
    "    poses_1_x = person_1_poses[:, :, 2]\n",
    "    poses_1_y = person_1_poses[:, :, 0]\n",
    "    poses_1_z = -person_1_poses[:, :, 1]\n",
    "    \n",
    "    poses_2_x = person_2_poses[:, :, 2]\n",
    "    poses_2_y = person_2_poses[:, :, 0]\n",
    "    poses_2_z = -person_2_poses[:, :, 1]\n",
    "    \n",
    "    def update(frame):\n",
    "        \n",
    "        scatt1._offsets3d = (poses_1_x[frame], poses_1_y[frame], poses_1_z[frame])\n",
    "        scatt2._offsets3d = (poses_2_x[frame], poses_2_y[frame], poses_2_z[frame])\n",
    "    \n",
    "        for line, (start, end) in zip(lines1, skeleton):\n",
    "            line.set_data([poses_1_x[frame, start], poses_1_x[frame, end]], [poses_1_y[frame, start], poses_1_y[frame, end]])\n",
    "            line.set_3d_properties([poses_1_z[frame, start], poses_1_z[frame, end]])\n",
    "    \n",
    "        for line, (start, end) in zip(lines2, skeleton):\n",
    "            line.set_data([poses_2_x[frame, start], poses_2_x[frame, end]], [poses_2_y[frame, start], poses_2_y[frame, end]])\n",
    "            line.set_3d_properties([poses_2_z[frame, start], poses_2_z[frame, end]])\n",
    "        \n",
    "        return scatt1, scatt2, *lines1, *lines2\n",
    "\n",
    "    plt.close(fig)\n",
    "    return FuncAnimation(fig, update, frames=range(len(poses_1_x)), interval=interval, blit=False)\n",
    "\n",
    "ani = animation(filtered_output[:200])\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566a954b-f862-4533-af60-2a2f0124bee7",
   "metadata": {},
   "source": [
    "## Fixing dancer indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2bcf77-a4f5-4cc7-ae26-adc2b5440e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the animation above, we can see the dancers sometimes exchange their positions\n",
    "# This phenomenom, however, is not captured by the indices as the simple study below show\n",
    "for i in range(int(len(filtered_output)/2)-1):\n",
    "    if filtered_output[i*2]['idx'] > filtered_output[i*2+1]['idx']:\n",
    "        print('Captured Inversion!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e92f6e-0d8d-4607-bd07-bdc28dd6a89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matching instances in throughout the entirety of the video\n",
    "for i in range(int(len(filtered_output)/2)-1):\n",
    "    person_1 = np.array(filtered_output[i*2]['pred_xyz_jts'])\n",
    "    to_compare_1 = np.array(filtered_output[(i+1)*2]['pred_xyz_jts'])\n",
    "    to_compare_2 = np.array(filtered_output[(i+1)*2+1]['pred_xyz_jts'])\n",
    "    \n",
    "    distance_1 = np.sum(np.linalg.norm(person_1 - to_compare_1))\n",
    "    distance_2 = np.sum(np.linalg.norm(person_1 - to_compare_2))\n",
    "\n",
    "    if distance_1 > distance_2:\n",
    "        change_order_aux = filtered_output[(i+1)*2+1].copy()\n",
    "        filtered_output[(i+1)*2+1] = filtered_output[(i+1)*2]\n",
    "        filtered_output[(i+1)*2] = change_order_aux\n",
    "\n",
    "        filtered_output[(i+1)*2]['idx'] = 1\n",
    "        filtered_output[(i+1)*2+1]['idx'] = 2\n",
    "\n",
    "# ani = animation(filtered_output)\n",
    "# HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbedcec-df54-4489-b2a6-2abfa948a278",
   "metadata": {},
   "source": [
    "## Smoothing data to handle jitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2b304c-264d-4744-8a6a-dce4cf647e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smoothing data to handle the jitter\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "person_1_poses = []\n",
    "person_2_poses = []\n",
    "for i, frame in enumerate(filtered_output):\n",
    "    if i%2 == 0:\n",
    "        person_1_poses.append(frame['pred_xyz_jts'])\n",
    "    else:\n",
    "        person_2_poses.append(frame['pred_xyz_jts'])\n",
    "person_1_poses = np.array(person_1_poses)\n",
    "person_2_poses = np.array(person_2_poses)\n",
    "\n",
    "# Savitzky-Golay filter\n",
    "def savgol_smoothness(poses, wl=12, po=3):\n",
    "    smoothed_data = np.zeros_like(poses)\n",
    "    \n",
    "    for joint in range(poses.shape[1]):\n",
    "        for axis in range(3):\n",
    "            smoothed_data[:, joint, axis] = savgol_filter(poses[:, joint, axis], wl, po)\n",
    "\n",
    "    return smoothed_data\n",
    "\n",
    "smoothed_data_1 = savgol_smoothness(person_1_poses)\n",
    "smoothed_data_2 = savgol_smoothness(person_2_poses)\n",
    "\n",
    "interleaved_array = np.zeros((2*smoothed_data_1.shape[0], smoothed_data_1.shape[1], smoothed_data_1.shape[2]))\n",
    "interleaved_array[0::2] = smoothed_data_1\n",
    "interleaved_array[1::2] = smoothed_data_2\n",
    "\n",
    "# print(\"###############################################################################################\")\n",
    "# print(\"#################################### Savitzky-Golay Filter ####################################\")\n",
    "# print(\"###############################################################################################\")\n",
    "# ani = animation(interleaved_array, np_flag=True)\n",
    "# HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d0819b-3d6d-4106-afd7-b64b103156f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.fftpack import dct, idct\n",
    "\n",
    "# Discrete cosine transform\n",
    "def dct_smoothness(poses, threshold=0.25):\n",
    "    smoothed_data = np.zeros_like(poses)\n",
    "    \n",
    "    for joint in range(poses.shape[1]):\n",
    "        for axis in range(3):\n",
    "            frequency_data = dct(poses[:, joint, axis], norm='ortho')\n",
    "            \n",
    "            frequency_data[int(threshold*len(frequency_data)):] = 0\n",
    "            \n",
    "            smoothed_data[:, joint, axis] = idct(frequency_data, norm='ortho')\n",
    "\n",
    "    return smoothed_data\n",
    "\n",
    "smoothed_data_1 = dct_smoothness(person_1_poses)\n",
    "smoothed_data_2 = dct_smoothness(person_2_poses)\n",
    "interleaved_array[0::2] = smoothed_data_1\n",
    "interleaved_array[1::2] = smoothed_data_2\n",
    "\n",
    "# print(\"###################################################################################################\")\n",
    "# print(\"#################################### Discrete Cosine Transform ####################################\")\n",
    "# print(\"###################################################################################################\")\n",
    "# ani = animation(interleaved_array, np_flag=True)\n",
    "# HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b1880c-940e-4b74-8c56-e252452a422b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D discrete cosine transform\n",
    "def dct_3d_smoothness(poses, threshold=0.25):\n",
    "    smoothed_data = np.zeros_like(poses)\n",
    "    \n",
    "    for joint in range(poses.shape[1]):\n",
    "        frequency_data = dct(poses[:, joint, :], axis=0, norm='ortho')\n",
    "        \n",
    "        frequency_data[int(threshold*len(frequency_data)):] = 0\n",
    "        \n",
    "        smoothed_data[:, joint, :] = idct(frequency_data, axis=0, norm='ortho')\n",
    "\n",
    "    return smoothed_data\n",
    "    \n",
    "smoothed_data_1 = dct_3d_smoothness(person_1_poses)\n",
    "smoothed_data_2 = dct_3d_smoothness(person_2_poses)\n",
    "interleaved_array[0::2] = smoothed_data_1\n",
    "interleaved_array[1::2] = smoothed_data_2\n",
    "\n",
    "print(\"######################################################################################################\")\n",
    "print(\"#################################### 3D Discrete Cosine Transform ####################################\")\n",
    "print(\"######################################################################################################\")\n",
    "ani = animation(interleaved_array[:200], np_flag=True)\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d8cb09-aef2-4f14-ac8d-e4255fc5f761",
   "metadata": {},
   "source": [
    "# Evaluating final poses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1688e761-a085-4c1c-9aa0-9818ab871d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing the original video, the initial mesh reconstuction and the extracted pose after full processing pipeline\n",
    "from IPython.display import Video, display\n",
    "\n",
    "video_1_path = './ilya_poses/0210-0220.mp4'\n",
    "video_2_path = './3d_pose_extraction/vis/mesh_video.mp4'\n",
    "# Video(video_path, width=640, height=480, embed=True)\n",
    "\n",
    "video_1_html = f\"\"\"\n",
    "<video width=\"520\" height=\"390\" controls>\n",
    "  <source src=\"{video_1_path}\" type=\"video/mp4\">\n",
    "  Your browser does not support the video tag.\n",
    "</video>\n",
    "\"\"\"\n",
    "\n",
    "video_2_html = f\"\"\"\n",
    "<video width=\"520\" height=\"390\" controls>\n",
    "  <source src=\"{video_2_path}\" type=\"video/mp4\">\n",
    "  Your browser does not support the video tag.\n",
    "</video>\n",
    "\"\"\"\n",
    "\n",
    "ani = animation(interleaved_array, np_flag=True, interval=35)\n",
    "ani_html = ani.to_jshtml()\n",
    "\n",
    "display(HTML(f\"\"\"\n",
    "<div style=\"display: flex; justify-content: space-around; margin-bottom: 20px;\">\n",
    "    <div style=\"flex: 1; padding: 10px;\">\n",
    "        {video_1_html}\n",
    "    </div>\n",
    "    <div style=\"flex: 1; padding: 10px;\">\n",
    "        {video_2_html}\n",
    "    </div>\n",
    "</div>\n",
    "<div style=\"text-align: center;\">\n",
    "    {ani_html}\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1371e997-8185-46e9-bc69-445616859277",
   "metadata": {},
   "source": [
    "# Saving data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26882a2f-e844-4ca5-85b8-70cf45df48dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving processed data to use on the model\n",
    "np.save('../model/data/pose_extraction_img_9085', interleaved_array)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
