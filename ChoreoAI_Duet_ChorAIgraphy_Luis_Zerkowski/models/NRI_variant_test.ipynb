{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9461899a-08ba-47f1-9032-2d3fd78893d7",
   "metadata": {},
   "source": [
    "<p style=\"font-size:16px;\">\n",
    "    This is a testing notebook for trying the NRI variant model before commiting to properly training it. Not only that, it allows for easier and more interactive visualizations once we have the trained weights.\n",
    "</p>\n",
    "<p style=\"font-size:16px;\">\n",
    "    It is divided into 3 main parts:\n",
    "    <ul style=\"font-size:16px;\">\n",
    "        <li><b>Data Preparation:</b> Loading data, visualizing it to better understand the inputs, preparing it with the appropriate shape and data structure for the models, and finally augmenting it for better training.</li>\n",
    "        <li><b>Building Models:</b> Defining the model and testing small training instances to check if the pipeline is working</li>\n",
    "        <li><b>Visualizations:</b> Implementing various types of visualizations given the trained model to benefit from their insights</li>\n",
    "    </ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c1c080-d09d-4ac9-abc9-1484ece30faa",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270b3671-83d3-468a-92b8-3e5822ff334c",
   "metadata": {},
   "source": [
    "<ul style=\"font-size:16px;\">\n",
    "    <li>Reading preprocessed data in an interleaved manner to extract data from both dancers separately. For better movement prediction and velocity estimation, we only read every 5th frame</li>\n",
    "    <li>Creating adjacencies by:\n",
    "        <ol>\n",
    "            <li>Initializing default skeleton for 29 joints</li>\n",
    "            <li>Repeating the process for the second dancer</li>\n",
    "            <li>Connecting every joint of a dancer to all joints on the other dancer (these are the ones we want to classify as existing or non-existing with the NRI model)</li>\n",
    "            <li>Duplicating the edges to make graph undirected</li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li>Visualizing both dancers with their default skeletons and also with the connecting edges</li>\n",
    "    <li>Estimating velocities based on 3D positions of consecutive frames</li>\n",
    "    <li>Preparing batches with PyTorch tensors of shape (batches, length_of_sequences, number_of_joints_from_both_dancers, 6D (position + velocity))</li>\n",
    "    <li>Creating training-validation split</li>\n",
    "    <li>To simplify the problem even further and better utilize the potential of the proposed architecture, in light of the amount of data available, a final sampling step of the dancers' joints is applied, resulting in fewer particles to be tracked</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5088fe34-38ab-47c3-9b2e-6ee4de2e6685",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.debugger import set_trace\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "\n",
    "# Reading data coming from the pre-processing pipeline and creating both dancers\n",
    "joint_poses = []\n",
    "for file in glob('./data/*.npy'):\n",
    "    interleaved_poses = np.load(file)\n",
    "    poses_1 = interleaved_poses[0::2]\n",
    "    poses_2 = interleaved_poses[1::2]\n",
    "\n",
    "    # Sampling frames for movement smoothness\n",
    "    joint_poses.append(np.concatenate((poses_1, poses_2), axis=1)[::5])\n",
    "    print('Joint poses {} shape: {}\\n'.format(file.split('/')[-1], joint_poses[-1].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed87f939-9f62-4029-a7f6-042d65c82131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building initial transposed edge index (adjacencies)\n",
    "edge_index_t = [[0, 1], [0, 2], [0, 3], [1, 4], [2, 5], [3, 6], [4, 7], [5, 8], [6, 9], [7, 10], [7, 27], [8, 11],\n",
    "                [8, 28], [9, 12], [9, 13], [9, 14], [12, 15], [13, 16], [14, 17], [15, 24], [16, 18], [17, 19],\n",
    "                [18, 20], [19, 21], [20, 22], [21, 23], [22, 25], [23, 26]]\n",
    "\n",
    "# Getting second person skeleton\n",
    "n_joints = int(joint_poses[0].shape[1]/2)\n",
    "init_skeleton_len = len(edge_index_t)\n",
    "for edge_index in range(init_skeleton_len):\n",
    "    edge_index_t.append([edge_index_t[edge_index][0]+n_joints, edge_index_t[edge_index][1]+n_joints])\n",
    "\n",
    "# Saving skeletons for visualization\n",
    "skeletons = edge_index_t.copy()\n",
    "\n",
    "# Fully connecting the two people\n",
    "for joint_1 in range(interleaved_poses.shape[1]):\n",
    "    for joint_2 in range(interleaved_poses.shape[1]):\n",
    "        edge_index_t.append([joint_1, joint_2+n_joints])\n",
    "\n",
    "# Making graph undirected\n",
    "full_skeleton_len = len(edge_index_t)\n",
    "for edge_index in range(full_skeleton_len):\n",
    "    edge_index_t.append([edge_index_t[edge_index][1], edge_index_t[edge_index][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc562866-f5ff-4b72-8359-2e96af00c10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing joint skeletons for random frame\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Plotting dancers\n",
    "ax1 = fig.add_subplot(121, projection=\"3d\")\n",
    "ax1.set_xlim([-1, 1])\n",
    "ax1.set_ylim([-1, 1])\n",
    "ax1.set_zlim([-1, 1])\n",
    "\n",
    "rand_frame = 0\n",
    "ax1.scatter(joint_poses[0][rand_frame, :n_joints, 2], joint_poses[0][rand_frame, :n_joints, 0], \\\n",
    "            -joint_poses[0][rand_frame, :n_joints, 1], color='red')\n",
    "ax1.scatter(joint_poses[0][rand_frame, n_joints:, 2], joint_poses[0][rand_frame, n_joints:, 0], \\\n",
    "            -joint_poses[0][rand_frame, n_joints:, 1], color='blue')\n",
    "\n",
    "for (start, end) in skeletons:\n",
    "    xs = [joint_poses[0][rand_frame, start, 2], joint_poses[0][rand_frame, end, 2]]\n",
    "    ys = [joint_poses[0][rand_frame, start, 0], joint_poses[0][rand_frame, end, 0]]\n",
    "    zs = [-joint_poses[0][rand_frame, start, 1], -joint_poses[0][rand_frame, end, 1]]\n",
    "    ax1.plot(xs, ys, zs, color='grey')\n",
    "\n",
    "# Plotting dancers with fully connected joints\n",
    "ax2 = fig.add_subplot(122, projection=\"3d\")\n",
    "ax2.set_xlim([-1, 1])\n",
    "ax2.set_ylim([-1, 1])\n",
    "ax2.set_zlim([-1, 1])\n",
    "\n",
    "ax2.scatter(joint_poses[0][rand_frame, :n_joints, 2], joint_poses[0][rand_frame, :n_joints, 0], \\\n",
    "            -joint_poses[0][rand_frame, :n_joints, 1], color='red')\n",
    "ax2.scatter(joint_poses[0][rand_frame, n_joints:, 2], joint_poses[0][rand_frame, n_joints:, 0], \\\n",
    "            -joint_poses[0][rand_frame, n_joints:, 1], color='blue')\n",
    "\n",
    "for (start, end) in edge_index_t[:int(len(edge_index_t)/2)]:\n",
    "    xs = [joint_poses[0][rand_frame, start, 2], joint_poses[0][rand_frame, end, 2]]\n",
    "    ys = [joint_poses[0][rand_frame, start, 0], joint_poses[0][rand_frame, end, 0]]\n",
    "    zs = [-joint_poses[0][rand_frame, start, 1], -joint_poses[0][rand_frame, end, 1]]\n",
    "    ax2.plot(xs, ys, zs, color='grey')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af157f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### Removed piece of code because now we rotate randomly on the fly while training ####################\n",
    "\n",
    "# # Defining rotation functions\n",
    "# def rotation_matrix_x(angle):\n",
    "#     c, s = np.cos(angle), np.sin(angle)\n",
    "    \n",
    "#     return np.array([[1, 0, 0],\n",
    "#                      [0, c, -s],\n",
    "#                      [0, s, c]])\n",
    "\n",
    "# def rotation_matrix_y(angle):\n",
    "#     c, s = np.cos(angle), np.sin(angle)\n",
    "    \n",
    "#     return np.array([[c, 0, s],\n",
    "#                      [0, 1, 0],\n",
    "#                      [-s, 0, c]])\n",
    "\n",
    "# def rotation_matrix_z(angle):\n",
    "#     c, s = np.cos(angle), np.sin(angle)\n",
    "    \n",
    "#     return np.array([[c, -s, 0],\n",
    "#                      [s, c, 0],\n",
    "#                      [0, 0, 1]])\n",
    "\n",
    "# def rotate_points(points, angle_x, angle_y, angle_z):\n",
    "#     Rx = rotation_matrix_x(angle_x)\n",
    "#     Ry = rotation_matrix_y(angle_y)\n",
    "#     Rz = rotation_matrix_z(angle_z)\n",
    "    \n",
    "#     rotation_matrix = Rz @ Ry @ Rx\n",
    "#     rotated_points = points @ rotation_matrix.T\n",
    "    \n",
    "#     return rotated_points\n",
    "\n",
    "# # Augmenting poses by adding random rotations (around Z-axis only)\n",
    "# rand_rot_num = 0\n",
    "# joint_poses_augmented = [joint_poses]\n",
    "# for r in range(rand_rot_num):\n",
    "#     joint_poses_rot = []\n",
    "    \n",
    "#     angle_x = 0 # 2*np.random.rand(1)[0]*np.pi\n",
    "#     angle_y = 2*np.random.rand(1)[0]*np.pi\n",
    "#     angle_z = 0 # 2*np.random.rand(1)[0]*np.pi\n",
    "    \n",
    "#     for choreo in joint_poses:\n",
    "#         choreo_aux = []\n",
    "        \n",
    "#         for frame in choreo:\n",
    "#             rotated_points = rotate_points(frame, angle_x, angle_y, angle_z)    \n",
    "#             choreo_aux.append(rotated_points)\n",
    "            \n",
    "#         joint_poses_rot.append(np.array(choreo_aux))\n",
    "\n",
    "#     joint_poses_augmented.append(joint_poses_rot)\n",
    "\n",
    "# # Visualizing rotated frame\n",
    "# fig = plt.figure(figsize=(12, 4))\n",
    "\n",
    "# ax1 = fig.add_subplot(131, projection=\"3d\")\n",
    "# ax1.set_xlim([-1, 1])\n",
    "# ax1.set_ylim([-1, 1])\n",
    "# ax1.set_zlim([-1, 1])\n",
    "\n",
    "# ax2 = fig.add_subplot(132, projection=\"3d\")\n",
    "# ax2.set_xlim([-1, 1])\n",
    "# ax2.set_ylim([-1, 1])\n",
    "# ax2.set_zlim([-1, 1])\n",
    "\n",
    "# ax3 = fig.add_subplot(133, projection=\"3d\")\n",
    "# ax3.set_xlim([-1, 1])\n",
    "# ax3.set_ylim([-1, 1])\n",
    "# ax3.set_zlim([-1, 1])\n",
    "\n",
    "# ax1.scatter(joint_poses[0][0, :n_joints, 2], joint_poses[0][0, :n_joints, 0], -joint_poses[0][0, :n_joints, 1], color='red')\n",
    "# ax1.scatter(joint_poses[0][0, n_joints:, 2], joint_poses[0][0, n_joints:, 0], -joint_poses[0][0, n_joints:, 1], color='blue')\n",
    "\n",
    "# ax2.scatter(joint_poses_augmented[1][0][0, :n_joints, 2], joint_poses_augmented[1][0][0, :n_joints, 0], \\\n",
    "#             -joint_poses_augmented[1][0][0, :n_joints, 1], color='red')\n",
    "# ax2.scatter(joint_poses_augmented[1][0][0, n_joints:, 2], joint_poses_augmented[1][0][0, n_joints:, 0], \\\n",
    "#             -joint_poses_augmented[1][0][0, n_joints:, 1], color='blue')\n",
    "\n",
    "# ax3.scatter(joint_poses_augmented[2][0][0, :n_joints, 2], joint_poses_augmented[2][0][0, :n_joints, 0], \\\n",
    "#             -joint_poses_augmented[2][0][0, :n_joints, 1], color='red')\n",
    "# ax3.scatter(joint_poses_augmented[2][0][0, n_joints:, 2], joint_poses_augmented[2][0][0, n_joints:, 0], \\\n",
    "#             -joint_poses_augmented[2][0][0, n_joints:, 1], color='blue')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c0e341",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Defining velocity function\n",
    "def compute_velocities(data, frame_gap=2):\n",
    "    velocities = data[frame_gap:] - data[:-frame_gap]\n",
    "\n",
    "    # Repeating velocity for final frames\n",
    "    padding = velocities[-1, :, :].repeat(frame_gap, 1, 1)\n",
    "\n",
    "    velocities = torch.cat((velocities, padding), dim=0)\n",
    "    \n",
    "    # Fixing velocity configuration\n",
    "    velocities = velocities[:, :, [2, 0, 1]]\n",
    "    velocities[:, :, 2] = -velocities[:, :, 2]\n",
    "    \n",
    "    return velocities\n",
    "\n",
    "# Estimating velocity of points\n",
    "frame_gap = 1\n",
    "velocities = []\n",
    "joint_poses_velo = []\n",
    "for joint_pose in [joint_poses]:\n",
    "    velocities_aux = []\n",
    "    joint_poses_velo_aux = []\n",
    "    \n",
    "    for choreo in joint_pose:\n",
    "        choreo = torch.Tensor(choreo)\n",
    "    \n",
    "        velocity_choreo = compute_velocities(choreo, frame_gap)\n",
    "        velocities_aux.append(velocity_choreo)\n",
    "        \n",
    "        joint_poses_velo_aux.append(torch.cat((choreo, velocity_choreo), dim=-1))\n",
    "\n",
    "    velocities.append(velocities_aux)\n",
    "    joint_poses_velo.append(joint_poses_velo_aux)\n",
    "\n",
    "# Visualizing velocity of points\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "\n",
    "ax1 = fig.add_subplot(121, projection=\"3d\")\n",
    "ax1.set_xlim([-1, 1])\n",
    "ax1.set_ylim([-1, 1])\n",
    "ax1.set_zlim([-1, 1])\n",
    "\n",
    "ax2 = fig.add_subplot(122, projection=\"3d\")\n",
    "ax2.set_xlim([-1, 1])\n",
    "ax2.set_ylim([-1, 1])\n",
    "ax2.set_zlim([-1, 1])\n",
    "\n",
    "rand_frame = 250\n",
    "ax1.scatter(joint_poses[1][rand_frame, :n_joints, 2], joint_poses[1][rand_frame, :n_joints, 0], \\\n",
    "            -joint_poses[1][rand_frame, :n_joints, 1], color='red')\n",
    "ax1.scatter(joint_poses[1][rand_frame+frame_gap, :n_joints, 2], joint_poses[1][rand_frame+frame_gap, :n_joints, 0], \\\n",
    "            -joint_poses[1][rand_frame+frame_gap, :n_joints, 1], color='orange')\n",
    "\n",
    "ax2.scatter(joint_poses[1][rand_frame, n_joints:, 2], joint_poses[1][rand_frame, n_joints:, 0], \\\n",
    "            -joint_poses[1][rand_frame, n_joints:, 1], color='blue')\n",
    "ax2.scatter(joint_poses[1][rand_frame+frame_gap, n_joints:, 2], joint_poses[1][rand_frame+frame_gap, n_joints:, 0], \\\n",
    "            -joint_poses[1][rand_frame+frame_gap, n_joints:, 1], color='purple')\n",
    "\n",
    "for point in range(2*n_joints):\n",
    "    x_start = joint_poses[1][rand_frame, point, 2]\n",
    "    y_start = joint_poses[1][rand_frame, point, 0]\n",
    "    z_start = -joint_poses[1][rand_frame, point, 1]\n",
    "\n",
    "    vx = velocities[0][1][rand_frame, point, 0]\n",
    "    vy = velocities[0][1][rand_frame, point, 1]\n",
    "    vz = velocities[0][1][rand_frame, point, 2]\n",
    "    \n",
    "    if point < n_joints:\n",
    "        ax1.quiver(x_start, y_start, z_start, vx, vy, vz, color='grey')\n",
    "    else:\n",
    "        ax2.quiver(x_start, y_start, z_start, vx, vy, vz, color='grey')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22feb2e4-ecf0-4dc7-b6ed-fa6e8116f440",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "seq_len = 16\n",
    "batches = []\n",
    "choreo_lens = []\n",
    "\n",
    "# Building non-overlapping sequences\n",
    "for joint_pose_velo in joint_poses_velo:\n",
    "    for choreo in joint_pose_velo:\n",
    "        choreo = torch.Tensor(choreo)\n",
    "    \n",
    "        num_seqs = choreo.shape[0] // seq_len\n",
    "        batches.append(torch.stack([choreo[i*seq_len:(i+1)*seq_len] for i in range(num_seqs)]))\n",
    "        choreo_lens.append(batches[-1].size(0))\n",
    "\n",
    "# # Building overlapping sequences\n",
    "# for joint_pose_velo in joint_poses_velo:\n",
    "#     for choreo in joint_pose_velo:\n",
    "#         choreo = torch.Tensor(choreo)\n",
    "    \n",
    "#         batches.append(torch.stack([choreo[i:i+seq_len] for i in range(len(choreo)-seq_len)]))\n",
    "#         choreo_lens.append(batches[-1].size(0))\n",
    "\n",
    "batches = torch.cat(batches, dim=0)\n",
    "\n",
    "# Fixing x, y, z configuration\n",
    "batches[:, :, :, [0, 1, 2]] = batches[:, :, :, [2, 0, 1]]\n",
    "batches[:, :, :, 2] = -batches[:, :, :, 2]\n",
    "\n",
    "# Balanced training-validation split\n",
    "train_split = []\n",
    "val_split = []\n",
    "train_batches = []\n",
    "val_batches = []\n",
    "split_percentage = 0.85\n",
    "next_choreo = 0\n",
    "for choreo_len in choreo_lens:\n",
    "    \n",
    "    train_split.append(int(split_percentage*choreo_len))\n",
    "    val_split.append(choreo_len - train_split[-1])\n",
    "    \n",
    "    train_batches.append(batches[next_choreo : next_choreo + train_split[-1]])\n",
    "    val_batches.append(batches[next_choreo + train_split[-1] : next_choreo + choreo_len])\n",
    "    \n",
    "    next_choreo += choreo_len\n",
    "\n",
    "train_batches = torch.cat(train_batches, dim=0)\n",
    "val_batches = torch.cat(val_batches, dim=0)\n",
    "\n",
    "# Printing all the data structures created\n",
    "print('Shape of tensor with all sequences: {}'.format(batches.shape))\n",
    "print('Length of each choreography: {}\\n'.format(choreo_lens))\n",
    "\n",
    "print('Shape of training data with all sequences: {}'.format(train_batches.shape))\n",
    "print('Length of each choreography in training dataset: {}\\n'.format(train_split))\n",
    "\n",
    "print('Shape of validation data with all sequences: {}'.format(val_batches.shape))\n",
    "print('Length of each choreography in validation dataset: {}\\n'.format(val_split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1c4ed9-2a66-484b-ad3e-d1019def1f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling dancers joints to simplify problem\n",
    "n_joints_sampled = 5\n",
    "sampled_joints_1 = np.random.choice(n_joints, n_joints_sampled)\n",
    "sampled_joints_2 = np.random.choice(n_joints, n_joints_sampled) + n_joints\n",
    "sampled_joints = np.concatenate([sampled_joints_1, sampled_joints_2])\n",
    "print(\"Sampled joints for dancer 1: {}, and dancer 2: {}\".format(sampled_joints_1, sampled_joints_2))\n",
    "\n",
    "sampled_batches = batches[:, :, sampled_joints, :]\n",
    "sampled_train_batches = train_batches[:, :, sampled_joints, :]\n",
    "sampled_val_batches = val_batches[:, :, sampled_joints, :]\n",
    "\n",
    "sampled_edge_index_t = []\n",
    "for (start, end) in edge_index_t[:len(edge_index_t)]:\n",
    "    if (start not in sampled_joints) or (end not in sampled_joints):\n",
    "        continue\n",
    "\n",
    "    sampled_edge_index_t.append([start, end])\n",
    "\n",
    "\n",
    "print('Shape of sampled data with all sequences: {}'.format(sampled_batches.shape))\n",
    "print('Shape of sampled training data with all sequences: {}'.format(sampled_train_batches.shape))\n",
    "print('Shape of sampled validation data with all sequences: {}'.format(sampled_val_batches.shape))\n",
    "\n",
    "# Plotting sampled dancers\n",
    "fig = plt.figure(figsize=(12, 4))\n",
    "ax1 = fig.add_subplot(121, projection=\"3d\")\n",
    "ax1.set_xlim([-1, 1])\n",
    "ax1.set_ylim([-1, 1])\n",
    "ax1.set_zlim([-1, 1])\n",
    "\n",
    "rand_frame = 0\n",
    "ax1.scatter(joint_poses[0][rand_frame, sampled_joints_1, 2], joint_poses[0][rand_frame, sampled_joints_1, 0], \\\n",
    "            -joint_poses[0][rand_frame, sampled_joints_1, 1], color='red')\n",
    "ax1.scatter(joint_poses[0][rand_frame, sampled_joints_2, 2], joint_poses[0][rand_frame, sampled_joints_2, 0], \\\n",
    "            -joint_poses[0][rand_frame, sampled_joints_2, 1], color='blue')\n",
    "\n",
    "for (start, end) in skeletons:\n",
    "    if (start not in sampled_joints) or (end not in sampled_joints):\n",
    "        continue\n",
    "    \n",
    "    xs = [joint_poses[0][rand_frame, start, 2], joint_poses[0][rand_frame, end, 2]]\n",
    "    ys = [joint_poses[0][rand_frame, start, 0], joint_poses[0][rand_frame, end, 0]]\n",
    "    zs = [-joint_poses[0][rand_frame, start, 1], -joint_poses[0][rand_frame, end, 1]]\n",
    "    ax1.plot(xs, ys, zs, color='grey')\n",
    "\n",
    "# Plotting sampled dancers with fully connected joints\n",
    "ax2 = fig.add_subplot(122, projection=\"3d\")\n",
    "ax2.set_xlim([-1, 1])\n",
    "ax2.set_ylim([-1, 1])\n",
    "ax2.set_zlim([-1, 1])\n",
    "\n",
    "ax2.scatter(joint_poses[0][rand_frame, sampled_joints_1, 2], joint_poses[0][rand_frame, sampled_joints_1, 0], \\\n",
    "            -joint_poses[0][rand_frame, sampled_joints_1, 1], color='red')\n",
    "ax2.scatter(joint_poses[0][rand_frame, sampled_joints_2, 2], joint_poses[0][rand_frame, sampled_joints_2, 0], \\\n",
    "            -joint_poses[0][rand_frame, sampled_joints_2, 1], color='blue')\n",
    "\n",
    "for (start, end) in edge_index_t[:int(len(edge_index_t)/2)]:\n",
    "    if (start not in sampled_joints) or (end not in sampled_joints):\n",
    "        continue\n",
    "        \n",
    "    xs = [joint_poses[0][rand_frame, start, 2], joint_poses[0][rand_frame, end, 2]]\n",
    "    ys = [joint_poses[0][rand_frame, start, 0], joint_poses[0][rand_frame, end, 0]]\n",
    "    zs = [-joint_poses[0][rand_frame, start, 1], -joint_poses[0][rand_frame, end, 1]]\n",
    "    ax2.plot(xs, ys, zs, color='grey')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b5080e-da7a-4d45-b129-410975ab7d39",
   "metadata": {},
   "source": [
    "# Bulding Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8930ee11-5b75-4fcf-b66c-9105dfd664ea",
   "metadata": {},
   "source": [
    "<p style=\"font-size:16px;\">Developed model:</p>\n",
    "<ul style=\"font-size:16px;\">\n",
    "    <li><b>NRI Variant:</b> Building a variation of the Neural Relational Inference (NRI) model for the task of graph structure learning given sequences of movement (interactions of joints)</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cc9966-a797-4254-9343-3f59895d5849",
   "metadata": {},
   "source": [
    "## NRI Variant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c508fbd8-1ebc-4da8-b71b-a38a3155d31b",
   "metadata": {},
   "source": [
    "<p style=\"font-size:16px;\">\n",
    "    This model is a variant of the Neural Relational Inference (NRI) model, which itself is an extension of the traditional Variational Autoencoder (VAE). The primary objective of the original model is to study particles that move together in a system without prior knowledge of their underlying relationships. By analyzing their movements, the model aims to estimate a graph structure that connects these particles. In our context, the particles are represented by the joints of dancers. Although we know the physical connections between joints within a dancer's body, this information is insufficient to understand the artistic relationships between two dancers, such as how their joints move together or in opposition.\n",
    "</p>\n",
    "\n",
    "<p style=\"font-size:16px;\">\n",
    "Since we lack a target graph structure that correctly identifies which joints are virtually connected during a dance performance, and given that this graph can change over time even within a performance (focusing on different body parts at different times), we employ self-supervising techniques.\n",
    "</p>\n",
    "\n",
    "<b style=\"font-size:18px;\">Model Overview</b>\n",
    "\n",
    "<p style=\"font-size:16px;\">\n",
    "The model consists of an encoder and a decoder, which play around with transforming node representations into edge representations and vice versa. This approach focuses on the dynamics of movements rather than fixed node embeddings. Since the encoder outputs edges (specifically, samples edges from the generated latent space), it is crucial to switch between these representations.\n",
    "</p>\n",
    "\n",
    "<p style=\"font-size:16px;\">\n",
    "Our implementation is similar to the NRI MLP-Encoder MLP-Decoder model, but with a few modifications:\n",
    "</p>\n",
    "\n",
    "<ul style=\"font-size:16px;\">\n",
    "    <li><b>Graph Convolutional Network (GCN):</b> We replaced some MLP layers with GCN layers to leverage the graph structure, improving the model's ability to capture relationships between joints. This change also helps us focus on a subset of edges that connect both dancers, rather than studying all particle relationships as in the original implementation. Additionally GCNs provide local feature aggregation and parameter sharing, important inductive biases for our context that could lead in enhanced generalization in a scenario with a \"dynamic\" (unknown) graph structure</li>\n",
    "    <li><b>GCN LSTM cells:</b> To utilize the recurrent structure crucial for sequence processing while maintaining graph information and GNN architecture, the classic LSTM cell has been reimplemented with GCN nodes. Currently only the decoder incorporates the recurrent component, which generates a final sequence embedding that the model uses to reconstruct the next frame</li>\n",
    "    <li><b>Use of Modern Libraries:</b> We utilize PyTorch Geometric for its advanced features and ease of use</li>\n",
    "</ul>\n",
    "\n",
    "<p style=\"font-size:16px;\">\n",
    "By incorporating these modifications, our model maintains the core principles of the original NRI model while theoretically enhancing its ability to generalize and adapt to the dynamic nature of dance performances.\n",
    "</p>\n",
    "\n",
    "<p style=\"font-size:16px;\">\n",
    "<b>Note:</b> To include data augmentation and improve model generalization, the training pipeline incorporates a data processing step that involves rotating batches of data. Each batch is rotated along the Z-axis by a randomly selected angle while maintaining the original X and Y-axis orientations for physical consistency. This approach helps prevent the model from overfitting to the dancers' absolute positions.\n",
    "</p>\n",
    "\n",
    "<b style=\"font-size:18px;\">Final Architecture</b>\n",
    "\n",
    "<ul style=\"font-size:16px;\">\n",
    "    <li>\n",
    "        <b>Encoder</b>\n",
    "        <ol style=\"font-size:16px;\">\n",
    "            <li>The encoder includes a GCN layer followed by a transformation of node representations into edge representations</li>\n",
    "            <li>We then use an MLP layer, batch normalization and dropout</li>\n",
    "            <li>After that, we convert edges back to nodes and apply another GCN layer</li>\n",
    "            <li>Nodes are then transformed back into edges, followed by another MLP with a skip connection from the dropout layer</li>\n",
    "            <li>And a final MLP layer outputs logits with (currently two) features representing the edge types (existing or non-existing)</li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li>\n",
    "        <b>Decoder</b>\n",
    "        <ol style=\"font-size:16px;\">\n",
    "            <li>Using the logits produced by the encoder, we sample a Gumbel-Softmax distribution. The idea is to approximate sampling in a continuous distribution and use Softmax to deal with the reparametrization trick, making the pipeline fully differentiable.</li>\n",
    "            <li>With the newly sampled edge index in hand, the decoder starts by passing data into a GRNN composed of our modified LSTM nodes with GCN layers, followed by a transformation of the final sequence embedding into edge representations</li>\n",
    "            <li>We then also use an MLP layer, batch normalization and dropout</li>\n",
    "            <li>After that, we convert edges back to nodes and apply a GCN layer to get a reconstructed frame</li>\n",
    "        </ol>\n",
    "    </li>\n",
    "</ul>\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de215173-f94b-4cf1-a6d9-9d08df62147f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Creating message passing matrices for receivers and senders - shape R^(E x N)\n",
    "def message_passing_matrices(n_joints, edge_index):\n",
    "    message_passing_in = torch.zeros((edge_index.size(1), n_joints))\n",
    "    message_passing_out = torch.zeros((edge_index.size(1), n_joints))\n",
    "\n",
    "    # Vectorizing message_passing matrices creation\n",
    "    edge_indices = torch.arange(edge_index.size(1))\n",
    "    message_passing_out[edge_indices, edge_index[0]] = 1.\n",
    "    message_passing_in[edge_indices, edge_index[1]] = 1.\n",
    "\n",
    "    return message_passing_in, message_passing_out\n",
    "\n",
    "\n",
    "# NRI VAE auxiliar functions to change between nodes and edges\n",
    "def node2edge(x, m_in, m_out):    \n",
    "    receivers = torch.matmul(m_in, x)\n",
    "    senders = torch.matmul(m_out, x)\n",
    "    edges = torch.cat([senders, receivers], dim=1)\n",
    "    \n",
    "    return edges\n",
    "\n",
    "def edge2node(x, m_in):\n",
    "    incoming = torch.matmul(m_in.t(), x)\n",
    "    \n",
    "    return incoming / incoming.size(0)\n",
    "\n",
    "\n",
    "# Gumbel-Softmax sampling function to allow for backpropagation with categorical distributions\n",
    "def gumbel_softmax_sample(logits, temp, hard=False):\n",
    "    y = F.gumbel_softmax(logits, tau=temp, hard=hard)\n",
    "    \n",
    "    return y\n",
    "\n",
    "\n",
    "# Computing KL Divergence for categorical distribution\n",
    "def gumbel_softmax_kl_divergence(logits, log_prior, batch_size):\n",
    "    q_y = F.softmax(logits, dim=-1)\n",
    "    kl_div = q_y * (F.log_softmax(logits, dim=-1) - log_prior)\n",
    "\n",
    "    # Normalizing by the batch size and number of edges\n",
    "    return kl_div.sum() / (batch_size * logits.size(0))\n",
    "\n",
    "\n",
    "# Initializing reconstruction losses\n",
    "nll_gaussian = nn.GaussianNLLLoss(reduction='mean') # Gaussian NLL\n",
    "mse = nn.MSELoss(reduction='mean') # MSE\n",
    "\n",
    "# Implementing LSTM variant with GCN layers\n",
    "class gcn_lstm_cell(nn.Module):\n",
    "    def __init__(self, n_in, n_out):\n",
    "        super(gcn_lstm_cell, self).__init__()\n",
    "\n",
    "        self.n_in = n_in\n",
    "        self.n_out = n_out\n",
    "        \n",
    "        # Rebuilding LSTM cell with GCN layers\n",
    "        self.gcn_i = GCNConv(n_in + n_out, n_out)\n",
    "        self.gcn_f = GCNConv(n_in + n_out, n_out)\n",
    "        self.gcn_o = GCNConv(n_in + n_out, n_out)\n",
    "        self.gcn_g = GCNConv(n_in + n_out, n_out)\n",
    "\n",
    "    def forward(self, x, h, c, edge_index):\n",
    "        # Concatenate input and hidden state\n",
    "        combined = torch.cat([x, h], dim=-1)\n",
    "        \n",
    "        # Compute gates\n",
    "        i = torch.sigmoid(self.gcn_i(combined, edge_index))\n",
    "        f = torch.sigmoid(self.gcn_f(combined, edge_index))\n",
    "        o = torch.sigmoid(self.gcn_o(combined, edge_index))\n",
    "        g = torch.tanh(self.gcn_g(combined, edge_index))\n",
    "        \n",
    "        # Compute new cell and hidden states\n",
    "        c_new = f*c + i*g\n",
    "        h_new = o*torch.tanh(c_new)\n",
    "        \n",
    "        return h_new, c_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d867d5-daba-4937-a72c-54d7d57ba967",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric.nn as geo_nn\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch.nn import Linear, BatchNorm1d, Dropout\n",
    "\n",
    "# Defining NRI encoder\n",
    "class nri_encoder(nn.Module):\n",
    "    def __init__(self, device, n_joints, edge_index_t, n_in, n_hid, n_out, do_prob=0.):\n",
    "        super(nri_encoder, self).__init__()\n",
    "\n",
    "        # Computing edge index given transposed edge index\n",
    "        self.edge_index = torch.Tensor(edge_index_t).t().long().to(device)\n",
    "\n",
    "        # Computing the message passing matrices\n",
    "        self.m_in, self.m_out = message_passing_matrices(n_joints, self.edge_index)\n",
    "        self.m_in = self.m_in.to(device)\n",
    "        self.m_out = self.m_out.to(device)\n",
    "\n",
    "        # Defining the network itself interleaving GCN and MLP layers\n",
    "        self.conv1 = GCNConv(n_in, n_hid, node_dim=1).to(device)\n",
    "        \n",
    "        self.mlp1 = Linear(n_hid*2, n_hid).to(device)\n",
    "        self.bnorm1 = BatchNorm1d(n_hid).to(device)\n",
    "        self.dropout1 = Dropout(do_prob).to(device)\n",
    "        \n",
    "        self.conv2 = GCNConv(n_hid, n_hid, node_dim=1).to(device)\n",
    "        \n",
    "        self.mlp2 = Linear(n_hid*3, n_hid).to(device)\n",
    "        self.bnorm2 = BatchNorm1d(n_hid).to(device)\n",
    "        \n",
    "        self.fc_out = Linear(n_hid, n_out).to(device)\n",
    "        \n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, Linear):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "                m.bias.data.fill_(0.1)\n",
    "\n",
    "            elif isinstance(m, GCNConv):\n",
    "                nn.init.xavier_normal_(m.lin.weight)\n",
    "                m.bias.data.fill_(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Rearranging shapes: [num_seqs, num_timesteps, num_atoms, num_dims] -> [num_seqs, num_atoms, num_timesteps*num_dims]\n",
    "        x = x.view(x.size(0), x.size(2), -1)\n",
    "\n",
    "        # Forward pass interleaving GCN layers, operations to switch from nodes to edges or vice-versa, and MLP layers\n",
    "        x = self.conv1(x, self.edge_index)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        edge_x = [node2edge(x_samp, self.m_in, self.m_out) for x_samp in x]\n",
    "        x = torch.stack(edge_x)\n",
    "        \n",
    "        x = self.mlp1(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.bnorm1(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        \n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        # Skip connection\n",
    "        x_skip = x.clone()\n",
    "\n",
    "        node_x = [edge2node(x_samp, self.m_in) for x_samp in x]\n",
    "        x = torch.stack(node_x)\n",
    "        \n",
    "        x = self.conv2(x, self.edge_index)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        edge_x = [node2edge(x_samp, self.m_in, self.m_out) for x_samp in x]\n",
    "        x = torch.stack(edge_x)\n",
    "        \n",
    "        x = torch.cat((x, x_skip), dim=2)\n",
    "        x = self.mlp2(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.bnorm2(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        return self.fc_out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f6022c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NRI recurrent encoder\n",
    "class nri_rec_encoder(nn.Module):\n",
    "    def __init__(self, device, n_joints, edge_index_t, n_in, n_hid, n_out, do_prob=0.):\n",
    "        super(nri_rec_encoder, self).__init__()\n",
    "        self.device = device\n",
    "\n",
    "        # Computing edge index given transposed edge index\n",
    "        self.edge_index = torch.Tensor(edge_index_t).t().long().to(device)\n",
    "\n",
    "        # Computing the message passing matrices\n",
    "        self.m_in, self.m_out = message_passing_matrices(n_joints, self.edge_index)\n",
    "        self.m_in = self.m_in.to(device)\n",
    "        self.m_out = self.m_out.to(device)\n",
    "\n",
    "        # Defining the network itself starting with GRNN and then MLP layers\n",
    "        self.grnn = gcn_lstm_cell(n_in, n_hid).to(device)\n",
    "        \n",
    "        self.mlp1 = Linear(n_hid*2, n_hid).to(device)\n",
    "        self.fc_out = Linear(n_hid, n_out).to(device)\n",
    "        \n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, Linear):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "                m.bias.data.fill_(0.1)\n",
    "\n",
    "            elif isinstance(m, GCNConv):\n",
    "                nn.init.xavier_normal_(m.lin.weight)\n",
    "                m.bias.data.fill_(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Rearranging shapes: [num_seqs, num_timesteps, num_atoms, num_dims]\n",
    "        num_seqs, num_timesteps, num_atoms, num_dims = x.shape\n",
    "\n",
    "        # Iterating through samples in the batch\n",
    "        h_batch = []\n",
    "        for x_b in x:\n",
    "            # Initializing cell and hidden states\n",
    "            h = torch.zeros(num_atoms, self.grnn.n_out).to(self.device)\n",
    "            c = torch.zeros(num_atoms, self.grnn.n_out).to(self.device)\n",
    "            \n",
    "            # Iterating through GRNN\n",
    "            for x_t in x_b:\n",
    "                h, c = self.grnn(x_t, h, c, self.edge_index)\n",
    "\n",
    "            h_batch.append(h)\n",
    "        h = torch.stack(h_batch)\n",
    "        \n",
    "        # Forward pass with an operation to switch from nodes to edges and MLP layers\n",
    "        edge_x = [node2edge(h_samp, self.m_in, self.m_out) for h_samp in h]\n",
    "        x = torch.stack(edge_x)\n",
    "        \n",
    "        x = self.mlp1(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        return self.fc_out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085bea43-dc37-4e9d-88b3-a5abfd98af75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining NRI decoder\n",
    "class nri_decoder(nn.Module):\n",
    "    def __init__(self, device, n_in, n_hid, n_out, do_prob=0.):\n",
    "        super(nri_decoder, self).__init__()\n",
    "\n",
    "        # Defining the network itself interleaving GCN and MLP layers\n",
    "        self.conv1 = GCNConv(n_in, n_hid).to(device)\n",
    "        \n",
    "        self.mlp1 = Linear(n_hid*2, n_hid).to(device)\n",
    "        # self.bnorm1 = BatchNorm1d(n_hid).to(device)\n",
    "        self.dropout1 = Dropout(do_prob).to(device)\n",
    "        \n",
    "        self.conv2 = GCNConv(n_hid, n_out).to(device)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, Linear):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "                m.bias.data.fill_(0.1)\n",
    "\n",
    "            elif isinstance(m, GCNConv):\n",
    "                nn.init.xavier_normal_(m.lin.weight)\n",
    "                m.bias.data.fill_(0.1)\n",
    "\n",
    "    def forward(self, x, edge_index, m_in, m_out):\n",
    "        # Rearranging shapes: [num_timesteps, num_atoms, num_dims] -> [num_atoms, num_timesteps*num_dims]\n",
    "        x = x.view(x.size(0), x.size(2), -1)\n",
    "\n",
    "        # Forward pass interleaving GCN layers, operations to switch from nodes to edges or vice-versa, and MLP layers\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = node2edge(x, m_in, m_out)\n",
    "        \n",
    "        x = self.mlp1(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # x = x.permute(0, 2, 1)\n",
    "        # x = self.bnorm1(x)\n",
    "        # x = x.permute(0, 2, 1)\n",
    "        \n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = edge2node(x, m_in)\n",
    "        \n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7707a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NRI recurrent decoder\n",
    "class nri_rec_decoder(nn.Module):\n",
    "    def __init__(self, device, n_in, n_hid, n_out, do_prob=0.):\n",
    "        super(nri_rec_decoder, self).__init__()\n",
    "        self.device = device\n",
    "        \n",
    "        # Defining the network itself starting with GRNN and then interleaving MLP and GCN layers\n",
    "        self.grnn = gcn_lstm_cell(n_in, n_hid).to(device)\n",
    "        \n",
    "        self.mlp1 = Linear(n_hid*2, n_hid).to(device)\n",
    "        self.dropout1 = Dropout(do_prob).to(device)\n",
    "        \n",
    "        self.conv1 = GCNConv(n_hid, n_out).to(device)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, Linear):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "                m.bias.data.fill_(0.1)\n",
    "\n",
    "            elif isinstance(m, GCNConv):\n",
    "                nn.init.xavier_normal_(m.lin.weight)\n",
    "                m.bias.data.fill_(0.1)\n",
    "\n",
    "    def forward(self, x, edge_index, m_in, m_out):\n",
    "        # [num_timesteps, num_atoms, num_dims]\n",
    "        num_timesteps, num_atoms, num_dims = x.shape\n",
    "        \n",
    "        # Initializing cell and hidden states\n",
    "        h = torch.zeros(num_atoms, self.grnn.n_out).to(self.device)\n",
    "        c = torch.zeros(num_atoms, self.grnn.n_out).to(self.device)\n",
    "        \n",
    "        # Iterating through GRNN\n",
    "        for x_t in x:\n",
    "            h, c = self.grnn(x_t, h, c, edge_index)\n",
    "\n",
    "        # Forward pass interleaving GCN layers, operations to switch from nodes to edges or vice-versa, and MLP layers\n",
    "        x = node2edge(h, m_in, m_out)\n",
    "        \n",
    "        x = self.mlp1(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = edge2node(x, m_in)\n",
    "        \n",
    "        x = self.conv1(x, edge_index)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0d757e-f97c-4d54-83bc-602b1008d17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining NRI VAE\n",
    "class nri_vae(nn.Module):\n",
    "    def __init__(self, device, n_joints, edge_index_t, n_in, n_hid, edge_types, n_out, tau, hard, do_prob=0., n_dims=6, num_cells=None):\n",
    "        super(nri_vae, self).__init__()\n",
    "\n",
    "        # Initializing encoder and decoder\n",
    "        if num_cells is None:\n",
    "            self.encoder = nri_encoder(device, n_joints, edge_index_t, n_in, n_hid, edge_types, do_prob)\n",
    "            self.decoder = nri_decoder(device, n_in, n_hid, n_out, do_prob)\n",
    "        else:\n",
    "            self.encoder = nri_encoder(device, n_joints, edge_index_t, n_in, n_hid, edge_types, do_prob)\n",
    "            # self.encoder = nri_rec_encoder(device, n_joints, edge_index_t, n_dims, n_hid, edge_types, do_prob)\n",
    "            self.decoder = nri_rec_decoder(device, n_dims, n_hid, n_out, do_prob)\n",
    "\n",
    "        # Saving variables that will be used by the forward pass\n",
    "        self.device = device\n",
    "        self.n_joints = n_joints\n",
    "        \n",
    "        self.tau = tau\n",
    "        self.hard = hard\n",
    "\n",
    "        self.edge_index_t = torch.Tensor(edge_index_t).to(device)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Computing logits for edges with encoder\n",
    "        logits = self.encoder(x)\n",
    "\n",
    "        # Sampling edge index classes using Gumbel-Softmax\n",
    "        y = gumbel_softmax_sample(logits, tau, hard)\n",
    "\n",
    "        # Getting sampled edges for every element in the batch\n",
    "        edge_index_dict = {i: [] for i in range(logits.size(0))}\n",
    "        edge_index_classes = torch.nonzero(y[:, :, -1])\n",
    "        for batch_element, edge in edge_index_classes:\n",
    "            edge_index_dict[batch_element.item()].append(edge.item())\n",
    "\n",
    "        recon_output = []\n",
    "        for k, v in edge_index_dict.items():\n",
    "            # Building edge_index for sampled edges\n",
    "            edge_index_samp = self.edge_index_t[v].t().long()\n",
    "\n",
    "            # Creating message passing matrices for decoder newly sampled edge index\n",
    "            decoder_m_in, decoder_m_out = message_passing_matrices(self.n_joints, edge_index_samp)\n",
    "            decoder_m_in = decoder_m_in.to(self.device)\n",
    "            decoder_m_out = decoder_m_out.to(self.device)\n",
    "\n",
    "            # Reconstructing sequences using decoder\n",
    "            recon_output.append(self.decoder(x[k], edge_index_samp, decoder_m_in, decoder_m_out))\n",
    "        \n",
    "        recon_output = torch.stack(recon_output)\n",
    "\n",
    "        return logits, recon_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c8b60a-bbca-4525-aeee-59a5be482c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing rotation function for fast rotation on full batch of data\n",
    "def rotation_matrix_z(angle):\n",
    "    c, s = torch.cos(angle), torch.sin(angle)\n",
    "    \n",
    "    return torch.tensor([[c, -s, 0],\n",
    "                         [s, c, 0],\n",
    "                         [0, 0, 1]], dtype=torch.float32)\n",
    "\n",
    "rotation = rotation_matrix_z(torch.tensor(2*np.random.rand(1)[0]*np.pi))\n",
    "rotated_batches_pos = torch.einsum('...ij,jk->...ik', train_batches[:16, :, :, :3], rotation)\n",
    "rotated_batches_vel = torch.einsum('...ij,jk->...ik', train_batches[:16, :, :, 3:], rotation)\n",
    "\n",
    "# Visualizing rotated frame\n",
    "fig = plt.figure(figsize=(12, 4))\n",
    "\n",
    "ax1 = fig.add_subplot(121, projection=\"3d\")\n",
    "ax1.set_xlim([-1, 1])\n",
    "ax1.set_ylim([-1, 1])\n",
    "ax1.set_zlim([-1, 1])\n",
    "\n",
    "ax2 = fig.add_subplot(122, projection=\"3d\")\n",
    "ax2.set_xlim([-1, 1])\n",
    "ax2.set_ylim([-1, 1])\n",
    "ax2.set_zlim([-1, 1])\n",
    "\n",
    "rand_seq = 1\n",
    "rand_frame = 1\n",
    "ax1.scatter(train_batches[rand_seq, rand_frame, :n_joints, 0], train_batches[rand_seq, rand_frame, :n_joints, 1], \\\n",
    "            train_batches[rand_seq, rand_frame, :n_joints, 2], color='red')\n",
    "ax1.scatter(train_batches[rand_seq, rand_frame, n_joints:, 0], train_batches[rand_seq, rand_frame, n_joints:, 1], \\\n",
    "            train_batches[rand_seq, rand_frame, n_joints:, 2], color='blue')\n",
    "\n",
    "ax2.scatter(rotated_batches_pos[rand_seq, rand_frame, :n_joints, 0], rotated_batches_pos[rand_seq, rand_frame, :n_joints, 1], \\\n",
    "            rotated_batches_pos[rand_seq, rand_frame, :n_joints, 2], color='red')\n",
    "ax2.scatter(rotated_batches_pos[rand_seq, rand_frame+1, :n_joints, 0], rotated_batches_pos[rand_seq, rand_frame+1, :n_joints, 1], \\\n",
    "            rotated_batches_pos[rand_seq, rand_frame+1, :n_joints, 2], color='orange')\n",
    "\n",
    "ax2.scatter(rotated_batches_pos[rand_seq, rand_frame, n_joints:, 0], rotated_batches_pos[rand_seq, rand_frame, n_joints:, 1], \\\n",
    "            rotated_batches_pos[rand_seq, rand_frame, n_joints:, 2], color='blue')\n",
    "ax2.scatter(rotated_batches_pos[rand_seq, rand_frame+1, n_joints:, 0], rotated_batches_pos[rand_seq, rand_frame+1, n_joints:, 1], \\\n",
    "            rotated_batches_pos[rand_seq, rand_frame+1, n_joints:, 2], color='purple')\n",
    "\n",
    "for point in range(2*n_joints):\n",
    "    x_start = rotated_batches_pos[rand_seq, rand_frame, point, 0]\n",
    "    y_start = rotated_batches_pos[rand_seq, rand_frame, point, 1]\n",
    "    z_start = rotated_batches_pos[rand_seq, rand_frame, point, 2]\n",
    "\n",
    "    vx = rotated_batches_vel[rand_seq, rand_frame, point, 0]\n",
    "    vy = rotated_batches_vel[rand_seq, rand_frame, point, 1]\n",
    "    vz = rotated_batches_vel[rand_seq, rand_frame, point, 2]\n",
    "    \n",
    "    ax2.quiver(x_start, y_start, z_start, vx, vy, vz, color='grey')\n",
    "    ax2.quiver(x_start, y_start, z_start, vx, vy, vz, color='grey')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240fc398-4390-4244-8ad3-d6ceca421a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "# Initializing all the hyperparameters and moving the required ones to device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "train_batches = train_batches.to(device)\n",
    "val_batches = val_batches.to(device)\n",
    "\n",
    "batch_size = batches.size(1)\n",
    "train_batches_cumsum = np.cumsum(np.array(train_split) // batch_size)\n",
    "val_batches_cumsum = np.cumsum(np.array(val_split) // batch_size)\n",
    "\n",
    "seq_len_in = batches.size(1)\n",
    "seq_len_out = 1\n",
    "n_joints = batches.size(2)\n",
    "dims = batches.size(3)\n",
    "\n",
    "hidden_dims = 32\n",
    "edge_types = 2\n",
    "\n",
    "tau = 0.5\n",
    "hard = True\n",
    "dropout = 0.1\n",
    "out_var = 5e-5\n",
    "\n",
    "prior = [0.7, 0.3]\n",
    "log_prior = torch.FloatTensor(np.log(prior)).unsqueeze(0).unsqueeze(0).to(device)\n",
    "\n",
    "epochs = 10\n",
    "lr = 3e-4\n",
    "lr_decay = 5\n",
    "gamma = 0.5\n",
    "\n",
    "# Initializing model\n",
    "model = nri_vae(device, n_joints, edge_index_t, seq_len_in*dims, hidden_dims, edge_types, seq_len_out*int(dims/2), \\\n",
    "                tau, hard, dropout, dims, seq_len_in)\n",
    "\n",
    "# Counting number of trainable parameters to compare to the dataset size\n",
    "print('Total number of trainable parameters: {}\\n'.format(sum(p.numel() for p in model.parameters() if p.requires_grad)))\n",
    "\n",
    "for n, p in model.named_parameters():\n",
    "    if p.requires_grad:\n",
    "        print('Layer {} has {} trainbale parameters'.format(n, p.numel()))\n",
    "\n",
    "# Initializing optimizer and scheduler\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=lr_decay, gamma=gamma)\n",
    "\n",
    "# Initializing lists to save losses across iterations\n",
    "kl_train = []\n",
    "recon_train = []\n",
    "loss_train = []\n",
    "kl_val = []\n",
    "recon_val = []\n",
    "loss_val = []\n",
    "\n",
    "# Initializing variables to save best model\n",
    "best_val_loss = torch.inf\n",
    "best_epoch = 0\n",
    "\n",
    "# Model iteration function\n",
    "def model_iteration(model, optimizer, scheduler, batches, batches_cumsum, beta, mode='train', recon_mode='nll'):\n",
    "    t = time.time()\n",
    "    \n",
    "    kl_aux = []\n",
    "    recon_aux = []\n",
    "    loss_aux = []\n",
    "\n",
    "    if mode == 'train':\n",
    "        model.train()\n",
    "    elif mode == 'val':\n",
    "        model.eval()\n",
    "\n",
    "    progress = 0.\n",
    "    choreo_cumsum_idx = 0\n",
    "    for idx in range(batches_cumsum[-1]):\n",
    "        if idx > progress*batches_cumsum[-1]:\n",
    "            print('Progress of training epoch: {:.1f}%'.format(progress*100))\n",
    "            progress += 0.1\n",
    "        \n",
    "        # Skipping last batch of a video, since the next batch belongs to the next video, not a delta_t of the movement\n",
    "        if idx == batches_cumsum[choreo_cumsum_idx]:\n",
    "            choreo_cumsum_idx += 1\n",
    "            continue\n",
    "\n",
    "        if mode == 'train':\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        batch = batches[idx*batch_size:(idx+1)*batch_size]\n",
    "\n",
    "        rotation = rotation_matrix_z(torch.tensor(2*np.random.rand(1)[0]*np.pi)).to(device)\n",
    "        rotated_batches_pos = torch.einsum('...ij,jk->...ik', batch[:, :, :, :3], rotation)\n",
    "        rotated_batches_vel = torch.einsum('...ij,jk->...ik', batch[:, :, :, 3:], rotation)\n",
    "        batch = torch.cat([rotated_batches_pos, rotated_batches_vel], dim=-1)\n",
    "        \n",
    "        # Non-overlapping sequences\n",
    "        # next_batch = batches[idx*batch_size+1:(idx+1)*batch_size+1]\n",
    "        \n",
    "        # Overlapping sequences\n",
    "        next_batch = batches[(idx+1)*batch_size:(idx+2)*batch_size]\n",
    "\n",
    "        rotated_batches_pos = torch.einsum('...ij,jk->...ik', next_batch[:, :, :, :3], rotation)\n",
    "        rotated_batches_vel = torch.einsum('...ij,jk->...ik', next_batch[:, :, :, 3:], rotation)\n",
    "        next_batch = torch.cat([rotated_batches_pos, rotated_batches_vel], dim=-1)\n",
    "        \n",
    "        logits, recon_output = model(batch)\n",
    "\n",
    "        kl_loss = gumbel_softmax_kl_divergence(logits, log_prior, batch_size)\n",
    "        \n",
    "        recon_output = recon_output.view(batch_size, seq_len_out, n_joints, int(dims/2))\n",
    "\n",
    "        if recon_mode == 'nll':\n",
    "            var_tensor = torch.full(recon_output.shape, out_var, device=device)\n",
    "            recon_loss = nll_gaussian(recon_output, next_batch[:, \\\n",
    "                                      frame_gap-1:frame_gap-1+seq_len_out, :, :int(dims/2)], var_tensor)\n",
    "        \n",
    "        elif recon_mode == 'mse':\n",
    "            recon_loss = mse(recon_output, next_batch[:, \\\n",
    "                             frame_gap-1:frame_gap-1+seq_len_out, :, :int(dims/2)])\n",
    "            \n",
    "        recon_loss = recon_loss / (recon_output.size(0) * recon_output.size(1) * recon_output.size(2))\n",
    "\n",
    "        if recon_mode == 'nll':\n",
    "            recon_coef = 0.0001\n",
    "        elif recon_mode == 'mse':\n",
    "            recon_coef = 1\n",
    "            \n",
    "        loss = beta*0.01*kl_loss + recon_coef*recon_loss\n",
    "        # loss = recon_coef*recon_loss\n",
    "\n",
    "        if mode == 'train':\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        kl_aux.append(0.01*kl_loss.item())\n",
    "        recon_aux.append(recon_coef*recon_loss.item())    \n",
    "        loss_aux.append(loss.data.item())\n",
    "\n",
    "    del batch, next_batch, logits, kl_loss, recon_loss\n",
    "    \n",
    "    kl_aux = torch.Tensor(kl_aux)\n",
    "    recon_aux = torch.Tensor(recon_aux)\n",
    "    loss_aux = torch.Tensor(loss_aux)\n",
    "    tqdm.write(f'Epoch: {epoch + 1:04d}, '\n",
    "               f'KL Loss ({mode}): {torch.mean(kl_aux):.4f}, '\n",
    "               f'Reconstruction Loss ({mode}): {torch.mean(recon_aux):.4f}, '\n",
    "               f'Combined Loss ({mode}): {torch.mean(loss_aux):.4f}, '\n",
    "               f'time: {time.time() - t:.4f}s')\n",
    "\n",
    "    if mode == 'train':\n",
    "        scheduler.step()\n",
    "\n",
    "    if mode == 'val':\n",
    "        global best_val_loss\n",
    "        global best_epoch\n",
    "        \n",
    "        if best_val_loss is torch.inf or torch.mean(loss_aux) < best_val_loss:    \n",
    "            best_val_loss = torch.mean(loss_aux)\n",
    "            best_epoch = epoch\n",
    "            \n",
    "            torch.save(model.state_dict(), 'best_weights/nri_parameters.pt')\n",
    "            tqdm.write(f'Epoch: {epoch + 1:04d}, Saving best parameters!')\n",
    "\n",
    "    if recon_mode == 'nll':\n",
    "        del var_tensor\n",
    "        \n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return kl_aux, recon_aux, loss_aux\n",
    "\n",
    "# Training loop\n",
    "for epoch in tqdm(range(epochs), desc='Training Epochs'):\n",
    "    # Beta coefficient to handle KL-Divergence vanishing gradients and balance reconstruction loss\n",
    "    beta = epoch % int(epochs*0.2) / (epochs*0.2)\n",
    "\n",
    "    kl_aux, recon_aux, loss_aux = model_iteration(model, optimizer, scheduler, train_batches, train_batches_cumsum, beta, 'train', 'nll')\n",
    "    \n",
    "    kl_train.append(torch.mean(kl_aux))\n",
    "    recon_train.append(torch.mean(recon_aux))\n",
    "    loss_train.append(torch.mean(loss_aux))\n",
    "\n",
    "    del kl_aux, recon_aux, loss_aux\n",
    "\n",
    "    with torch.no_grad():\n",
    "        kl_aux, recon_aux, loss_aux = model_iteration(model, optimizer, scheduler, val_batches, val_batches_cumsum, beta, 'val', 'nll')\n",
    "    \n",
    "    kl_val.append(torch.mean(kl_aux))\n",
    "    recon_val.append(torch.mean(recon_aux))\n",
    "    loss_val.append(torch.mean(loss_aux))\n",
    "\n",
    "    del kl_aux, recon_aux, loss_aux\n",
    "\n",
    "print(\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae68e12-880b-4578-8fb9-35fc7b6e9fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plotting training losses\n",
    "ax1 = fig.add_subplot(231)\n",
    "ax1.plot(kl_train, color='blue')\n",
    "\n",
    "ax1 = fig.add_subplot(232)\n",
    "ax1.plot(recon_train, color='blue')\n",
    "\n",
    "ax1 = fig.add_subplot(233)\n",
    "ax1.plot(loss_train, color='blue')\n",
    "\n",
    "# Plotting validation losses\n",
    "ax1 = fig.add_subplot(234)\n",
    "ax1.plot(kl_val, color='orange')\n",
    "\n",
    "ax1 = fig.add_subplot(235)\n",
    "ax1.plot(recon_val, color='orange')\n",
    "\n",
    "ax1 = fig.add_subplot(236)\n",
    "ax1.plot(loss_val, color='orange')\n",
    "\n",
    "# Adding column labels\n",
    "fig.text(0.22, 0.96, 'KL Loss', ha='center', fontsize=14)\n",
    "fig.text(0.53, 0.96, 'Reconstruction Loss', ha='center', fontsize=14)\n",
    "fig.text(0.85, 0.96, 'Combined Loss', ha='center', fontsize=14)\n",
    "\n",
    "# Adding row labels\n",
    "fig.text(0.02, 0.73, 'Training', va='center', rotation='vertical', fontsize=14)\n",
    "fig.text(0.02, 0.30, 'Validation', va='center', rotation='vertical', fontsize=14)\n",
    "\n",
    "plt.tight_layout(rect=[0.05, 0.05, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da38150-3db7-4d19-a40f-15849bf50b85",
   "metadata": {},
   "source": [
    "# Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ddf755-602b-45fb-8216-bb989bf85473",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Increasing animation memory limit\n",
    "from matplotlib import rcParams\n",
    "rcParams['animation.embed_limit'] = 100\n",
    "\n",
    "# Animation function\n",
    "def animation(sequence, skeleton=None, interval=100):\n",
    "    fig = plt.figure(figsize=(16, 12))\n",
    "    \n",
    "    ax = fig.add_subplot(111, projection=\"3d\")\n",
    "    ax.set_xlim([-1, 1])\n",
    "    ax.set_ylim([-1, 1])\n",
    "    ax.set_zlim([-1, 1])\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_zticks([])\n",
    "\n",
    "    ms = 70\n",
    "    scatt1 = ax.scatter([], [], [], color='red', s=ms)\n",
    "    scatt2 = ax.scatter([], [], [], color='blue', s=ms)\n",
    "\n",
    "    if skeleton is not None:\n",
    "        lw = 4\n",
    "        lines = [ax.plot([], [], [], 'gray', linewidth=lw)[0] for _ in skeleton]\n",
    "\n",
    "    # sequence_x = sequence[:, :, 2]\n",
    "    # sequence_y = sequence[:, :, 0]\n",
    "    # sequence_z = -sequence[:, :, 1]\n",
    "    \n",
    "    def update(frame):\n",
    "        \n",
    "        scatt1._offsets3d = (sequence_x[frame, :n_joints], sequence_y[frame, :n_joints], sequence_z[frame, :n_joints])\n",
    "        scatt2._offsets3d = (sequence_x[frame, n_joints:], sequence_y[frame, n_joints:], sequence_z[frame, n_joints:])\n",
    "    \n",
    "        if skeleton is not None:\n",
    "            for line, (start, end) in zip(lines, skeleton):\n",
    "                line.set_data([sequence_x[frame, start], sequence_x[frame, end]], [sequence_y[frame, start], sequence_y[frame, end]])\n",
    "                line.set_3d_properties([sequence_z[frame, start], sequence_z[frame, end]])\n",
    "            \n",
    "            return scatt1, scatt2, *lines\n",
    "\n",
    "        return scatt1, scatt2\n",
    "\n",
    "    plt.close(fig)\n",
    "    return FuncAnimation(fig, update, frames=range(len(sequence_x)), interval=interval, blit=False)\n",
    "\n",
    "# ani = animation(train_batches[sequence], skeletons, interval=100)\n",
    "# HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f880668-b607-4ad7-b669-a37038bdd49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading best model\n",
    "model.load_state_dict(torch.load('best_weights/nri_parameters.pt'))\n",
    "\n",
    "# Choosing sequence based on the selected video and frame, and preparing n_joints for plotting\n",
    "choreo_idx = 0\n",
    "offset = 0\n",
    "if choreo_idx != 0:\n",
    "    offset = np.cumsum(train_split)[choreo_idx-1]\n",
    "\n",
    "frame = 0\n",
    "sequence = frame // seq_len_in\n",
    "\n",
    "n_joints = int(batches.size(2)/2)\n",
    "\n",
    "# Getting predicted edges from a sequence\n",
    "logits = model.encoder(train_batches[offset + sequence].unsqueeze(0)).squeeze(0)\n",
    "\n",
    "y = gumbel_softmax_sample(logits, tau, False)\n",
    "\n",
    "# # Confidence threshold\n",
    "# confidence = 0.95\n",
    "# edge_index_classes = torch.where(y[:, 1] > confidence)[0]\n",
    "\n",
    "# Top % edges\n",
    "percentage = 0.01\n",
    "k = int(len(edge_index_t)*percentage)\n",
    "edge_index_classes = torch.topk(y[:, 1], k)[1]\n",
    "\n",
    "edge_index_samp = torch.Tensor(edge_index_t).to(device)[edge_index_classes].t().long()\n",
    "# print('Amount of edges sampled with confidence {}%: {}'.format(int(confidence*100), len(edge_index_classes)))\n",
    "print('Top {}% edges: {}'.format(int(percentage*100), len(edge_index_classes)))\n",
    "\n",
    "# Getting reconstructed frame\n",
    "decoder_m_in, decoder_m_out = message_passing_matrices(n_joints*2, edge_index_samp)\n",
    "decoder_m_in = decoder_m_in.to(device)\n",
    "decoder_m_out = decoder_m_out.to(device)\n",
    "\n",
    "recon_output = model.decoder(train_batches[offset + sequence], \\\n",
    "                                 edge_index_samp, decoder_m_in, decoder_m_out)\n",
    "recon_output = recon_output.view(seq_len_out, n_joints*2, int(dims/2)).cpu().detach().numpy()\n",
    "\n",
    "if seq_len_out == 1:\n",
    "    recon_output = recon_output.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61ee5b3-6005-47c6-8c84-9b00b2a065f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting dancers\n",
    "fig = plt.figure(figsize=(12, 4))\n",
    "\n",
    "ax1 = fig.add_subplot(121, projection=\"3d\")\n",
    "ax1.set_xlim([-1, 1])\n",
    "ax1.set_ylim([-1, 1])\n",
    "ax1.set_zlim([-1, 1])\n",
    "ax1.set_xticks([])\n",
    "ax1.set_yticks([])\n",
    "ax1.set_zticks([])\n",
    "\n",
    "ax1.scatter(joint_poses[choreo_idx][frame, :n_joints, 2], joint_poses[choreo_idx][frame, :n_joints, 0], \\\n",
    "            -joint_poses[choreo_idx][frame, :n_joints, 1], color='red')\n",
    "ax1.scatter(joint_poses[choreo_idx][frame, n_joints:, 2], joint_poses[choreo_idx][frame, n_joints:, 0], \\\n",
    "            -joint_poses[choreo_idx][frame, n_joints:, 1], color='blue')\n",
    "\n",
    "for (start, end) in skeletons:\n",
    "    xs = [joint_poses[choreo_idx][frame, start, 2], joint_poses[choreo_idx][frame, end, 2]]\n",
    "    ys = [joint_poses[choreo_idx][frame, start, 0], joint_poses[choreo_idx][frame, end, 0]]\n",
    "    zs = [-joint_poses[choreo_idx][frame, start, 1], -joint_poses[choreo_idx][frame, end, 1]]\n",
    "    ax1.plot(xs, ys, zs, color='grey')\n",
    "\n",
    "# Plotting dancers with sampled edges\n",
    "ax2 = fig.add_subplot(122, projection=\"3d\")\n",
    "ax2.set_xlim([-1, 1])\n",
    "ax2.set_ylim([-1, 1])\n",
    "ax2.set_zlim([-1, 1])\n",
    "ax2.set_xticks([])\n",
    "ax2.set_yticks([])\n",
    "ax2.set_zticks([])\n",
    "\n",
    "ax2.scatter(joint_poses[choreo_idx][frame, :n_joints, 2], joint_poses[choreo_idx][frame, :n_joints, 0], \\\n",
    "            -joint_poses[choreo_idx][frame, :n_joints, 1], color='red')\n",
    "ax2.scatter(joint_poses[choreo_idx][frame, n_joints:, 2], joint_poses[choreo_idx][frame, n_joints:, 0], \\\n",
    "            -joint_poses[choreo_idx][frame, n_joints:, 1], color='blue')\n",
    "\n",
    "for (start, end) in edge_index_samp.t():\n",
    "    xs = [joint_poses[choreo_idx][frame, start, 2], joint_poses[choreo_idx][frame, end, 2]]\n",
    "    ys = [joint_poses[choreo_idx][frame, start, 0], joint_poses[choreo_idx][frame, end, 0]]\n",
    "    zs = [-joint_poses[choreo_idx][frame, start, 1], -joint_poses[choreo_idx][frame, end, 1]]\n",
    "    ax2.plot(xs, ys, zs, color='grey')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8a4a72-cb55-4abf-a150-f621976b5aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting reconstructed frame, if frame was reconstructed instead of sequence\n",
    "if seq_len_out == 1:\n",
    "    fig = plt.figure(figsize=(12, 4))\n",
    "    ax1 = fig.add_subplot(111, projection=\"3d\")\n",
    "    ax1.set_xlim([-0.5, 0.5])\n",
    "    ax1.set_ylim([-0.5, 0.5])\n",
    "    ax1.set_zlim([-0.5, 0.5])\n",
    "    ax1.set_xticks([])\n",
    "    ax1.set_yticks([])\n",
    "    ax1.set_zticks([])\n",
    "    \n",
    "    ax1.scatter(recon_output[:n_joints, 0], recon_output[:n_joints, 1], recon_output[:n_joints, 2], color='red')\n",
    "    ax1.scatter(recon_output[n_joints:, 0], recon_output[n_joints:, 1], recon_output[n_joints:, 2], color='blue')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa502d9c-c36f-440c-8fb8-afd57aac0444",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video, display\n",
    "\n",
    "# Getting dataset back to CPU\n",
    "train_batches = train_batches.cpu()\n",
    "\n",
    "# Evaluating the edge prediction by watching the dance sequence with and without the connections\n",
    "ani = animation(train_batches[offset + sequence], skeletons, interval=100)\n",
    "ani_html_no_edge_pred = ani.to_jshtml()\n",
    "\n",
    "ani = animation(train_batches[offset + sequence], edge_index_samp, interval=100)\n",
    "ani_html_edge_pred = ani.to_jshtml()\n",
    "\n",
    "# Evaluating reconstruction by watching next dance sequence and the predicted one\n",
    "ani = animation(train_batches[offset + sequence + 1], skeletons, interval=100)\n",
    "ani_html_no_mov_pred = ani.to_jshtml()\n",
    "\n",
    "ani = animation(recon_output, skeletons, interval=100)\n",
    "ani_html_mov_pred = ani.to_jshtml()\n",
    "\n",
    "display(HTML(f\"\"\"\n",
    "<div style=\"display: flex; justify-content: space-around;\">\n",
    "    <div style=\"margin: 5px;\">\n",
    "        {ani_html_no_edge_pred}\n",
    "    </div>\n",
    "    <div style=\"margin: 5px;\">\n",
    "        {ani_html_edge_pred}\n",
    "    </div>\n",
    "</div>\n",
    "<div style=\"display: flex; justify-content: space-around;\">\n",
    "    <div style=\"margin: 5px;\">\n",
    "        {ani_html_no_mov_pred}\n",
    "    </div>\n",
    "    <div style=\"margin: 5px;\">\n",
    "        {ani_html_mov_pred}\n",
    "    </div>\n",
    "</div>\n",
    "\"\"\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
