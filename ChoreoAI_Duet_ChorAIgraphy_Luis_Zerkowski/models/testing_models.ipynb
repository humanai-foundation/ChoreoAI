{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9461899a-08ba-47f1-9032-2d3fd78893d7",
   "metadata": {},
   "source": [
    "<p style=\"font-size:16px;\">\n",
    "    This is a testing notebook for trying the models before commiting to properly training them. Not only that, it allows for easier and more interactive visualizations once we have the trained weights.\n",
    "</p>\n",
    "<p style=\"font-size:16px;\">\n",
    "    It is divided into 3 main parts:\n",
    "    <ul style=\"font-size:16px;\">\n",
    "        <li><b>Data Preparation:</b> Loading data, visualizing it to better understand the inputs and preparing it with the appropriate shape and data structure for the models</li>\n",
    "        <li><b>Building Models:</b> Defining the models and testing small training instances to check they're working</li>\n",
    "        <li><b>Visualizations:</b> Implementing various types of visualizations given the trained models to benefit from their insights</li>\n",
    "    </ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c1c080-d09d-4ac9-abc9-1484ece30faa",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270b3671-83d3-468a-92b8-3e5822ff334c",
   "metadata": {},
   "source": [
    "<ul style=\"font-size:16px;\">\n",
    "    <li>Reading preprocessed data in an interleaved manner to extract data from both dancers separately</li>\n",
    "    <li>Creating adjacencies by:\n",
    "        <ol>\n",
    "            <li>Initializing default skeleton for 29 joints</li>\n",
    "            <li>Repeating the process for the second dancer</li>\n",
    "            <li>Connecting every joint of a dancer to all joints on the other dancer (these are the ones we want to classify as existing or non-existing with the NRI model)</li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li>Visualizing both dancers with their default skeletons and also with the connecting edges</li>\n",
    "    <li>Preparing batches with PyTorch tensors of shape (batches, length_of_sequences, number_of_joints_from_both_dancers, 3D)</li>\n",
    "    <li>Creating training-validation split</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5088fe34-38ab-47c3-9b2e-6ee4de2e6685",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.debugger import set_trace\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "\n",
    "# Reading data coming from the pre-processing pipeline and creating both dancers\n",
    "joint_poses = []\n",
    "for file in glob('./data/*.npy'):\n",
    "    interleaved_poses = np.load(file)\n",
    "    poses_1 = interleaved_poses[0::2]\n",
    "    poses_2 = interleaved_poses[1::2]\n",
    "    \n",
    "    joint_poses.append(np.concatenate((poses_1, poses_2), axis=1))\n",
    "    print('Joint poses {} shape: {}\\n'.format(file.split('/')[-1], joint_poses[-1].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed87f939-9f62-4029-a7f6-042d65c82131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building initial transposed edge index (adjacencies)\n",
    "edge_index_t = [[0, 1], [0, 2], [0, 3], [1, 4], [2, 5], [3, 6], [4, 7], [5, 8], [6, 9], [7, 10], [7, 27], [8, 11],\n",
    "                [8, 28], [9, 12], [9, 13], [9, 14], [12, 15], [13, 16], [14, 17], [15, 24], [16, 18], [17, 19],\n",
    "                [18, 20], [19, 21], [20, 22], [21, 23], [22, 25], [23, 26]]\n",
    "\n",
    "# Getting second person skeleton\n",
    "n_joints = int(joint_poses[0].shape[1]/2)\n",
    "init_skeleton_len = len(edge_index_t)\n",
    "for edge_index in range(init_skeleton_len):\n",
    "    edge_index_t.append([edge_index_t[edge_index][0]+n_joints, edge_index_t[edge_index][1]+n_joints])\n",
    "\n",
    "# Saving skeletons for visualization\n",
    "skeletons = edge_index_t.copy()\n",
    "\n",
    "# Fully connecting the two people\n",
    "for joint_1 in range(interleaved_poses.shape[1]):\n",
    "    for joint_2 in range(interleaved_poses.shape[1]):\n",
    "        edge_index_t.append([joint_1, joint_2+n_joints])\n",
    "\n",
    "# Making graph undirected\n",
    "full_skeleton_len = len(edge_index_t)\n",
    "for edge_index in range(full_skeleton_len):\n",
    "    edge_index_t.append([edge_index_t[edge_index][1], edge_index_t[edge_index][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc562866-f5ff-4b72-8359-2e96af00c10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing joint skeletons for random frame\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Plotting dancers\n",
    "ax1 = fig.add_subplot(121, projection=\"3d\")\n",
    "ax1.set_xlim([-1, 1])\n",
    "ax1.set_ylim([-1, 1])\n",
    "ax1.set_zlim([-1, 1])\n",
    "\n",
    "rand_frame = 42\n",
    "ax1.scatter(joint_poses[0][rand_frame, :n_joints, 2], joint_poses[0][rand_frame, :n_joints, 0], \\\n",
    "            -joint_poses[0][rand_frame, :n_joints, 1], color='red')\n",
    "ax1.scatter(joint_poses[0][rand_frame, n_joints:, 2], joint_poses[0][rand_frame, n_joints:, 0], \\\n",
    "            -joint_poses[0][rand_frame, n_joints:, 1], color='blue')\n",
    "\n",
    "for (start, end) in skeletons:\n",
    "    xs = [joint_poses[0][rand_frame, start, 2], joint_poses[0][rand_frame, end, 2]]\n",
    "    ys = [joint_poses[0][rand_frame, start, 0], joint_poses[0][rand_frame, end, 0]]\n",
    "    zs = [-joint_poses[0][rand_frame, start, 1], -joint_poses[0][rand_frame, end, 1]]\n",
    "    ax1.plot(xs, ys, zs, color='grey')\n",
    "\n",
    "# Plotting dancers with fully connected joints\n",
    "ax2 = fig.add_subplot(122, projection=\"3d\")\n",
    "ax2.set_xlim([-1, 1])\n",
    "ax2.set_ylim([-1, 1])\n",
    "ax2.set_zlim([-1, 1])\n",
    "\n",
    "ax2.scatter(joint_poses[0][rand_frame, :n_joints, 2], joint_poses[0][rand_frame, :n_joints, 0], \\\n",
    "            -joint_poses[0][rand_frame, :n_joints, 1], color='red')\n",
    "ax2.scatter(joint_poses[0][rand_frame, n_joints:, 2], joint_poses[0][rand_frame, n_joints:, 0], \\\n",
    "            -joint_poses[0][rand_frame, n_joints:, 1], color='blue')\n",
    "\n",
    "for (start, end) in edge_index_t[:int(len(edge_index_t)/2)]:\n",
    "    xs = [joint_poses[0][rand_frame, start, 2], joint_poses[0][rand_frame, end, 2]]\n",
    "    ys = [joint_poses[0][rand_frame, start, 0], joint_poses[0][rand_frame, end, 0]]\n",
    "    zs = [-joint_poses[0][rand_frame, start, 1], -joint_poses[0][rand_frame, end, 1]]\n",
    "    ax2.plot(xs, ys, zs, color='grey')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22feb2e4-ecf0-4dc7-b6ed-fa6e8116f440",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "seq_len = 32\n",
    "batches = []\n",
    "choreo_lens = []\n",
    "\n",
    "# # Building non-overlapping sequences\n",
    "# for choreo in joint_poses:\n",
    "#     choreo = torch.Tensor(choreo)\n",
    "\n",
    "#     num_seqs = choreo.shape[0] // seq_len\n",
    "#     batches.append(torch.stack([choreo[i*seq_len:(i+1)*seq_len] for i in range(num_seqs)]))\n",
    "#     choreo_lens.append(batches[-1].size(0))\n",
    "\n",
    "# Building overlapping sequences\n",
    "for choreo in joint_poses:\n",
    "    choreo = torch.Tensor(choreo)\n",
    "\n",
    "    batches.append(torch.stack([choreo[i:i+seq_len] for i in range(len(choreo)-seq_len)]))\n",
    "    choreo_lens.append(batches[-1].size(0))\n",
    "\n",
    "batches = torch.cat(batches, dim=0)\n",
    "\n",
    "# Balanced training-validation split\n",
    "train_split = []\n",
    "val_split = []\n",
    "train_batches = []\n",
    "val_batches = []\n",
    "next_choreo = 0\n",
    "for choreo_len in choreo_lens:\n",
    "    \n",
    "    train_split.append(int(0.85*choreo_len))\n",
    "    val_split.append(choreo_len - train_split[-1])\n",
    "    \n",
    "    train_batches.append(batches[next_choreo : next_choreo + train_split[-1]])\n",
    "    val_batches.append(batches[next_choreo + train_split[-1] : next_choreo + choreo_len])\n",
    "    \n",
    "    next_choreo += choreo_len\n",
    "\n",
    "train_batches = torch.cat(train_batches, dim=0)\n",
    "val_batches = torch.cat(val_batches, dim=0)\n",
    "\n",
    "# Printing all the data structures created\n",
    "print('Shape of tensor with all sequences: {}'.format(batches.shape))\n",
    "print('Length of each choreography: {}\\n'.format(choreo_lens))\n",
    "\n",
    "print('Shape of training data with all sequences: {}'.format(train_batches.shape))\n",
    "print('Length of each choreography in training dataset: {}\\n'.format(train_split))\n",
    "\n",
    "print('Shape of validation data with all sequences: {}'.format(val_batches.shape))\n",
    "print('Length of each choreography in validation dataset: {}\\n'.format(val_split))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b5080e-da7a-4d45-b129-410975ab7d39",
   "metadata": {},
   "source": [
    "# Bulding Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8930ee11-5b75-4fcf-b66c-9105dfd664ea",
   "metadata": {},
   "source": [
    "<p style=\"font-size:16px;\">The developed models were:</p>\n",
    "<ul style=\"font-size:16px;\">\n",
    "    <li><b>NRI Variant:</b> Building a GCN variation of the Neural Relational Inference (NRI) model for the task of graph structure learning given sequences of movement (interactions of joints)</li>\n",
    "    <li>Temporal model (?)</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cc9966-a797-4254-9343-3f59895d5849",
   "metadata": {},
   "source": [
    "## NRI Variant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c508fbd8-1ebc-4da8-b71b-a38a3155d31b",
   "metadata": {},
   "source": [
    "<p style=\"font-size:16px;\">\n",
    "    This model is a variant of the Neural Relational Inference (NRI) model, which itself is an extension of the traditional Variational Autoencoder (VAE). The primary objective of the original model is to study particles that move together in a system without prior knowledge of their underlying relationships. By analyzing their movements, the model aims to estimate a graph structure that connects these particles. In our context, the particles are represented by the joints of dancers. Although we know the physical connections between joints within a dancer's body, this information is insufficient to understand the artistic relationships between two dancers, such as how their joints move together or in opposition.\n",
    "</p>\n",
    "\n",
    "<p style=\"font-size:16px;\">\n",
    "Since we lack a target graph structure that correctly identifies which joints are virtually connected during a dance performance, and given that this graph can change over time within a performance (focusing on different body parts at different times), we employ self-supervising techniques.\n",
    "</p>\n",
    "\n",
    "<b style=\"font-size:18px;\">Model Overview</b>\n",
    "\n",
    "<p style=\"font-size:16px;\">\n",
    "The model consists of an encoder and a decoder, which play around with transforming node representations into edge representations and vice versa. This approach focuses on the dynamics of movements rather than fixed node embeddings. Since the encoder outputs edges (specifically, samples edges from the generated latent space), it is crucial to switch between these representations.\n",
    "</p>\n",
    "\n",
    "<p style=\"font-size:16px;\">\n",
    "Our implementation is similar to the NRI MLP-Encoder MLP-Decoder model, but with a couple of modifications:\n",
    "</p>\n",
    "\n",
    "<ul style=\"font-size:16px;\">\n",
    "    <li><b>Graph Convolutional Network (GCN):</b> We replaced some MLP layers with GCN layers to leverage the graph structure, improving the model's ability to capture relationships between joints. This change also helps us focus on a subset of edges that connect both dancers, rather than studying all particle relationships as in the original implementation. Additionally GCNs provide local feature aggregation and parameter sharing, important inductive biases for our context and resulting in enhanced generalization in a scenario with a \"dynamic\" (unknown) graph structure</li>\n",
    "    <li><b>Predicting Sequences:</b> Since our data doesn't include velocity of the points, only their 3D position, the Markovian property doesn't hold. Therefore, to predict movement, we started reconstructing entire sequences</li>\n",
    "    <li><b>Use of Modern Libraries:</b> We utilize PyTorch Geometric for its advanced features and ease of use</li>\n",
    "</ul>\n",
    "\n",
    "<p style=\"font-size:16px;\">\n",
    "By incorporating these modifications, our model maintains the core principles of the original NRI model while enhancing its ability to generalize and adapt to the dynamic nature of dance performances.\n",
    "</p>\n",
    "\n",
    "<b style=\"font-size:18px;\">Final Architecture</b>\n",
    "\n",
    "<ul style=\"font-size:16px;\">\n",
    "    <li>\n",
    "        <b>Encoder</b>\n",
    "        <ol style=\"font-size:16px;\">\n",
    "            <li>The encoder includes a GCN layer followed by a transformation of node representations into edge representations</li>\n",
    "            <li>We then use an MLP layer, batch normalization and dropout</li>\n",
    "            <li>After that, we convert edges back to nodes and apply another GCN layer</li>\n",
    "            <li>Nodes are then transformed back into edges, followed by another MLP with a skip connection from the dropout layer</li>\n",
    "            <li>And a final MLP layer outputs logits with two features representing the edge types (existing or non-existing)</li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li>\n",
    "        <b>Decoder</b>\n",
    "        <ol style=\"font-size:16px;\">\n",
    "            <li>Using the logits produced by the encoder, we hard sample a Gumbel-Softmax distribution. The idea is to approximate sampling in a continuous distribution and use Softmax to deal with the reparametrization trick, making the pipeline fully differentiable.</li>\n",
    "            <li>With the newly sampled edge index in hand, the decoder starts by passing data into a GCN layer followed by a transformation of node representations into edge representations</li>\n",
    "            <li>We then also use an MLP layer, batch normalization and dropout</li>\n",
    "            <li>After that, we convert edges back to nodes and apply another GCN layer to get the reconstructed sequence</li>\n",
    "        </ol>\n",
    "    </li>\n",
    "</ul>\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de215173-f94b-4cf1-a6d9-9d08df62147f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Creating message passing matrices for receivers and senders - shape R^(E x N)\n",
    "def message_passing_matrices(n_joints, edge_index):\n",
    "    message_passing_in = torch.zeros((edge_index.size(1), n_joints))\n",
    "    message_passing_out = torch.zeros((edge_index.size(1), n_joints))\n",
    "\n",
    "    for j in range(edge_index.size(1)):\n",
    "        message_passing_out[j, int(edge_index[0, j])] = 1.\n",
    "        message_passing_in[j, int(edge_index[1, j])] = 1.\n",
    "\n",
    "    return message_passing_in, message_passing_out\n",
    "\n",
    "# NRI VAE auxiliar functions to change between nodes and edges\n",
    "def node2edge(x, m_in, m_out):    \n",
    "    receivers = torch.matmul(m_in, x)\n",
    "    senders = torch.matmul(m_out, x)\n",
    "    edges = torch.cat([senders, receivers], dim=1)\n",
    "    \n",
    "    return edges\n",
    "\n",
    "\n",
    "def edge2node(x, m_in):\n",
    "    incoming = torch.matmul(m_in.t(), x)\n",
    "    \n",
    "    return incoming / incoming.size(0)\n",
    "\n",
    "\n",
    "# Gumbel-Softmax sampling function to allow for backpropagation with categorical distributions\n",
    "def gumbel_softmax_sample(logits, temp, hard=False):\n",
    "    y = F.gumbel_softmax(logits, tau=temp, hard=hard)\n",
    "    \n",
    "    return y\n",
    "\n",
    "\n",
    "# Computing KL Divergence for categorical distribution\n",
    "def gumbel_softmax_kl_divergence(logits, log_prior, batch_size):\n",
    "    q_y = F.softmax(logits, dim=-1)\n",
    "    kl_div = q_y * (F.log_softmax(logits, dim=-1) - log_prior)\n",
    "\n",
    "    # Normalizing by the batch size and number of edges\n",
    "    return kl_div.sum() / (batch_size * logits.size(0))\n",
    "\n",
    "\n",
    "# Initializing reconstruction losses\n",
    "nll_gaussian = nn.GaussianNLLLoss(reduction='sum') # Gaussian NLL\n",
    "mse = nn.MSELoss(reduction='sum') # MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d867d5-daba-4937-a72c-54d7d57ba967",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric.nn as geo_nn\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch.nn import Linear, BatchNorm1d, Dropout\n",
    "\n",
    "# Defining NRI encoder\n",
    "class nri_encoder(nn.Module):\n",
    "    def __init__(self, device, n_joints, edge_index_t, n_in, n_hid, n_out, do_prob=0.):\n",
    "        super(nri_encoder, self).__init__()\n",
    "\n",
    "        # Computing edge index given transposed edge index\n",
    "        self.edge_index = torch.Tensor(edge_index_t).t().long().to(device)\n",
    "\n",
    "        # Computing the message passing matrices\n",
    "        self.m_in, self.m_out = message_passing_matrices(n_joints, self.edge_index)\n",
    "        self.m_in = self.m_in.to(device)\n",
    "        self.m_out = self.m_out.to(device)\n",
    "\n",
    "        # Defining the network itself interleaving GCN and MLP layers\n",
    "        self.conv1 = GCNConv(n_in, n_hid).to(device)\n",
    "        \n",
    "        self.mlp1 = Linear(n_hid*2, n_hid).to(device)\n",
    "        self.bnorm1 = BatchNorm1d(n_hid).to(device)\n",
    "        self.dropout1 = Dropout(do_prob).to(device)\n",
    "        \n",
    "        self.conv2 = GCNConv(n_hid, n_hid).to(device)\n",
    "        \n",
    "        self.mlp2 = Linear(n_hid*3, n_hid).to(device)\n",
    "        self.bnorm2 = BatchNorm1d(n_hid).to(device)\n",
    "        \n",
    "        self.fc_out = Linear(n_hid, n_out).to(device)\n",
    "        \n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, Linear):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "                m.bias.data.fill_(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Rearranging shapes: [num_seqs, num_timesteps, num_atoms, num_dims] -> [num_seqs, num_atoms, num_timesteps*num_dims]\n",
    "        x = x.view(x.size(0), x.size(2), -1)\n",
    "\n",
    "        # Forward pass interleaving GCN layers, operations to switch from nodes to edges or vice-versa, and MLP layers\n",
    "        x = self.conv1(x, self.edge_index)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        edge_x = [node2edge(x_samp, self.m_in, self.m_out) for x_samp in x]\n",
    "        x = torch.stack(edge_x)\n",
    "        \n",
    "        x = self.mlp1(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.bnorm1(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        \n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        # Skip connection\n",
    "        x_skip = x.clone()\n",
    "\n",
    "        node_x = [edge2node(x_samp, self.m_in) for x_samp in x]\n",
    "        x = torch.stack(node_x)\n",
    "        \n",
    "        x = self.conv2(x, self.edge_index)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        edge_x = [node2edge(x_samp, self.m_in, self.m_out) for x_samp in x]\n",
    "        x = torch.stack(edge_x)\n",
    "        \n",
    "        x = torch.cat((x, x_skip), dim=2)\n",
    "        x = self.mlp2(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.bnorm2(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        return self.fc_out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085bea43-dc37-4e9d-88b3-a5abfd98af75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining NRI decoder\n",
    "class nri_decoder(nn.Module):\n",
    "    def __init__(self, device, n_in, n_hid, n_out, do_prob=0.):\n",
    "        super(nri_decoder, self).__init__()\n",
    "\n",
    "        # Defining the network itself interleaving GCN and MLP layers\n",
    "        self.conv1 = GCNConv(n_in, n_hid).to(device)\n",
    "        \n",
    "        self.mlp1 = Linear(n_hid*2, n_hid).to(device)\n",
    "        self.bnorm1 = BatchNorm1d(n_hid).to(device)\n",
    "        self.dropout1 = Dropout(do_prob).to(device)\n",
    "        \n",
    "        self.conv2 = GCNConv(n_hid, n_out).to(device)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, Linear):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "                m.bias.data.fill_(0.1)\n",
    "\n",
    "    def forward(self, x, edge_index, m_in, m_out):\n",
    "        # Rearranging shapes: [num_seqs, num_timesteps, num_atoms, num_dims] -> [num_seqs, num_atoms, num_timesteps*num_dims]\n",
    "        x = x.view(x.size(0), x.size(2), -1)\n",
    "\n",
    "        # Forward pass interleaving GCN layers, operations to switch from nodes to edges or vice-versa, and MLP layers\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        edge_x = [node2edge(x_samp, m_in, m_out) for x_samp in x]\n",
    "        x = torch.stack(edge_x)\n",
    "        \n",
    "        x = self.mlp1(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.bnorm1(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        \n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        node_x = [edge2node(x_samp, m_in) for x_samp in x]\n",
    "        x = torch.stack(node_x)\n",
    "        \n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0d757e-f97c-4d54-83bc-602b1008d17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining NRI VAE\n",
    "class nri_vae(nn.Module):\n",
    "    def __init__(self, device, n_joints, edge_index_t, n_in, n_hid, edge_types, n_out, tau, hard, do_prob=0.):\n",
    "        super(nri_vae, self).__init__()\n",
    "\n",
    "        # Initializing encoder and decoder\n",
    "        self.encoder = nri_encoder(device, n_joints, edge_index_t, n_in, n_hid, edge_types, do_prob)\n",
    "        self.decoder = nri_decoder(device, n_in, n_hid, n_out, do_prob)\n",
    "\n",
    "        # Saving variables that will be used by the forward pass\n",
    "        self.device = device\n",
    "        self.n_joints = n_joints\n",
    "        \n",
    "        self.tau = tau\n",
    "        self.hard = hard\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Computing logits for edges with encoder\n",
    "        logits = self.encoder(x)\n",
    "\n",
    "        # Sampling edge index classes using Gumbel-Softmax. Since we are using only two types of edges at the moment,\n",
    "        # existent or non-existent, we create a newly sampled edge index for the decoder to use\n",
    "        edge_index_classes = gumbel_softmax_sample(logits, tau, hard)\n",
    "        edge_index_samp = torch.Tensor(edge_index_t).to(x.device)[torch.where(edge_index_classes[:, 1])[0]].t().long()\n",
    "\n",
    "        # Creating message passing matrices for decoder newly sampled edge index\n",
    "        decoder_m_in, decoder_m_out = message_passing_matrices(self.n_joints, edge_index_samp)\n",
    "        decoder_m_in = decoder_m_in.to(self.device)\n",
    "        decoder_m_out = decoder_m_out.to(self.device)\n",
    "\n",
    "        # Reconstructing sequences using decoder\n",
    "        recon_output = self.decoder(x, edge_index_samp, decoder_m_in, decoder_m_out)\n",
    "\n",
    "        return logits, recon_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240fc398-4390-4244-8ad3-d6ceca421a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "# Initializing all the hyperparameters and moving the required ones to device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "train_batches = train_batches.to(device)\n",
    "val_batches = val_batches.to(device)\n",
    "\n",
    "batch_size = 8\n",
    "train_batches_cumsum = np.cumsum(np.array(train_split) // batch_size)\n",
    "val_batches_cumsum = np.cumsum(np.array(val_split) // batch_size)\n",
    "\n",
    "seq_len_in = batches.size(1)\n",
    "seq_len_out = 4\n",
    "n_joints = batches.size(2)\n",
    "dims = batches.size(3)\n",
    "\n",
    "hidden_dims = 256\n",
    "edge_types = 2\n",
    "\n",
    "tau = 0.5\n",
    "hard = True\n",
    "dropout = 0.1\n",
    "out_var = 5e-5\n",
    "\n",
    "prior = [0.9, 0.1]\n",
    "log_prior = torch.FloatTensor(np.log(prior)).unsqueeze(0).unsqueeze(0).to(device)\n",
    "\n",
    "epochs = 500\n",
    "lr = 5e-4\n",
    "lr_decay = 50\n",
    "gamma = 0.5\n",
    "\n",
    "# Initializing model, optimizer and scheduler\n",
    "model = nri_vae(device, n_joints, edge_index_t, seq_len_in*dims, hidden_dims, edge_types, seq_len_out*dims, tau, hard, dropout)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=lr_decay, gamma=gamma)\n",
    "\n",
    "# Initializing lists to save losses across iterations\n",
    "kl_train = []\n",
    "recon_train = []\n",
    "loss_train = []\n",
    "kl_val = []\n",
    "recon_val = []\n",
    "loss_val = []\n",
    "\n",
    "# Initializing variables to save best model\n",
    "best_val_loss = torch.inf\n",
    "best_epoch = 0\n",
    "\n",
    "# Model iteration function\n",
    "def model_iteration(model, optimizer, scheduler, batches, batches_cumsum, beta, mode='train', recon_mode='nll'):\n",
    "    t = time.time()\n",
    "    \n",
    "    kl_aux = []\n",
    "    recon_aux = []\n",
    "    loss_aux = []\n",
    "\n",
    "    if mode == 'train':\n",
    "        model.train()\n",
    "    elif mode == 'val':\n",
    "        model.eval()\n",
    "    \n",
    "    choreo_cumsum_idx = 0\n",
    "    for idx in range(batches_cumsum[-1]):\n",
    "        # Skipping last batch of a video, since the next batch belongs to the next video, not a delta_t of the movement\n",
    "        if idx == batches_cumsum[choreo_cumsum_idx]:\n",
    "            choreo_cumsum_idx += 1\n",
    "            continue\n",
    "\n",
    "        if mode == 'train':\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        batch = batches[idx*batch_size:(idx+1)*batch_size]\n",
    "        # batch = batch.to(device)\n",
    "        logits, recon_output = model(batch)\n",
    "\n",
    "        kl_loss = gumbel_softmax_kl_divergence(logits, log_prior, batch_size)\n",
    "        \n",
    "        recon_output = recon_output.view(batch_size, seq_len_out, n_joints, dims)\n",
    "\n",
    "        if recon_mode == 'nll':\n",
    "            var_tensor = torch.full(recon_output.shape, out_var, device=device)\n",
    "            recon_loss = nll_gaussian(recon_output, batches[(idx+1)*batch_size:(idx+2)*batch_size, :seq_len_out, :, :], var_tensor)    \n",
    "        elif recon_mode == 'mse':\n",
    "            recon_loss = mse(recon_output, batches[(idx+1)*batch_size:(idx+2)*batch_size, :seq_len_out, :, :])\n",
    "            \n",
    "        recon_loss = recon_loss / (recon_output.size(0) * recon_output.size(1) * recon_output.size(2))\n",
    "\n",
    "        if recon_mode == 'nll':\n",
    "            recon_coef = 0.0001\n",
    "        elif recon_mode == 'mse':\n",
    "            recon_coef = 1\n",
    "            \n",
    "        loss = 0.01*kl_loss + recon_coef*recon_loss\n",
    "        # loss = recon_coef*recon_loss\n",
    "\n",
    "        if mode == 'train':\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        kl_aux.append(0.01*kl_loss)\n",
    "        recon_aux.append(recon_coef*recon_loss)    \n",
    "        loss_aux.append(loss.data.item())\n",
    "\n",
    "    kl_aux = torch.Tensor(kl_aux)\n",
    "    recon_aux = torch.Tensor(recon_aux)\n",
    "    loss_aux = torch.Tensor(loss_aux)\n",
    "    tqdm.write(f'Epoch: {epoch + 1:04d}, '\n",
    "               f'KL Loss ({mode}): {torch.mean(kl_aux):.4f}, '\n",
    "               f'Reconstruction Loss ({mode}): {torch.mean(recon_aux):.4f}, '\n",
    "               f'Combined Loss ({mode}): {torch.mean(loss_aux):.4f}, '\n",
    "               f'time: {time.time() - t:.4f}s')\n",
    "\n",
    "    if mode == 'train':\n",
    "        scheduler.step()\n",
    "\n",
    "    if mode == 'val':\n",
    "        global best_val_loss\n",
    "        global best_epoch\n",
    "        \n",
    "        if best_val_loss is torch.inf or torch.mean(loss_aux) < best_val_loss:    \n",
    "            best_val_loss = torch.mean(loss_aux)\n",
    "            best_epoch = epoch\n",
    "            \n",
    "            torch.save(model.state_dict(), 'best_weights/nri_parameters.pt')\n",
    "            tqdm.write(f'Epoch: {epoch + 1:04d}, Saving best parameters!')\n",
    "\n",
    "    del var_tensor\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return kl_aux, recon_aux, loss_aux\n",
    "\n",
    "# Training loop\n",
    "# for epoch in tqdm(range(epochs), desc='Training Epochs'):\n",
    "#     # Beta coefficient to handle KL-Divergence vanishing gradients and balance reconstruction loss\n",
    "#     beta = epoch % int(epochs*0.1) / (epochs*0.1)\n",
    "\n",
    "#     kl_aux, recon_aux, loss_aux = model_iteration(model, optimizer, scheduler, train_batches, train_batches_cumsum, beta, 'train', 'nll')\n",
    "    \n",
    "#     kl_train.append(torch.mean(kl_aux))\n",
    "#     recon_train.append(torch.mean(recon_aux))\n",
    "#     loss_train.append(torch.mean(loss_aux))\n",
    "\n",
    "#     kl_aux, recon_aux, loss_aux = model_iteration(model, optimizer, scheduler, val_batches, val_batches_cumsum, beta, 'val', 'nll')\n",
    "    \n",
    "#     kl_val.append(torch.mean(kl_aux))\n",
    "#     recon_val.append(torch.mean(recon_aux))\n",
    "#     loss_val.append(torch.mean(loss_aux))\n",
    "\n",
    "# print(\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae68e12-880b-4578-8fb9-35fc7b6e9fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plotting training losses\n",
    "ax1 = fig.add_subplot(231)\n",
    "ax1.plot(kl_train, color='blue')\n",
    "\n",
    "ax1 = fig.add_subplot(232)\n",
    "ax1.plot(recon_train, color='blue')\n",
    "\n",
    "ax1 = fig.add_subplot(233)\n",
    "ax1.plot(loss_train, color='blue')\n",
    "\n",
    "# Plotting validation losses\n",
    "ax1 = fig.add_subplot(234)\n",
    "ax1.plot(kl_val, color='orange')\n",
    "\n",
    "ax1 = fig.add_subplot(235)\n",
    "ax1.plot(recon_val, color='orange')\n",
    "\n",
    "ax1 = fig.add_subplot(236)\n",
    "ax1.plot(loss_val, color='orange')\n",
    "\n",
    "# Adding column labels\n",
    "fig.text(0.22, 0.96, 'KL Loss', ha='center', fontsize=14)\n",
    "fig.text(0.53, 0.96, 'Reconstruction Loss', ha='center', fontsize=14)\n",
    "fig.text(0.85, 0.96, 'Combined Loss', ha='center', fontsize=14)\n",
    "\n",
    "# Adding row labels\n",
    "fig.text(0.02, 0.73, 'Training', va='center', rotation='vertical', fontsize=14)\n",
    "fig.text(0.02, 0.30, 'Validation', va='center', rotation='vertical', fontsize=14)\n",
    "\n",
    "plt.tight_layout(rect=[0.05, 0.05, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e282b7-d864-4cbb-aed3-c0663fdfe966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting number of trainable parameters to compare to the dataset size\n",
    "print('Total number of trainable parameters: {}\\n'.format(sum(p.numel() for p in model.parameters() if p.requires_grad)))\n",
    "\n",
    "for n, p in model.named_parameters():\n",
    "    if p.requires_grad:\n",
    "        print('Layer {} has {} trainbale parameters'.format(n, p.numel()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da38150-3db7-4d19-a40f-15849bf50b85",
   "metadata": {},
   "source": [
    "# Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ddf755-602b-45fb-8216-bb989bf85473",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Increasing animation memory limit\n",
    "from matplotlib import rcParams\n",
    "rcParams['animation.embed_limit'] = 100\n",
    "\n",
    "# Animation function\n",
    "def animation(sequence, skeleton=None, interval=100):\n",
    "    fig = plt.figure(figsize=(16, 12))\n",
    "    \n",
    "    ax = fig.add_subplot(111, projection=\"3d\")\n",
    "    ax.set_xlim([-1, 1])\n",
    "    ax.set_ylim([-1, 1])\n",
    "    ax.set_zlim([-1, 1])\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_zticks([])\n",
    "\n",
    "    ms = 70\n",
    "    scatt1 = ax.scatter([], [], [], color='red', s=ms)\n",
    "    scatt2 = ax.scatter([], [], [], color='blue', s=ms)\n",
    "\n",
    "    if skeleton is not None:\n",
    "        lw = 4\n",
    "        lines = [ax.plot([], [], [], 'gray', linewidth=lw)[0] for _ in skeleton]\n",
    "\n",
    "    sequence_x = sequence[:, :, 2]\n",
    "    sequence_y = sequence[:, :, 0]\n",
    "    sequence_z = -sequence[:, :, 1]\n",
    "    \n",
    "    def update(frame):\n",
    "        \n",
    "        scatt1._offsets3d = (sequence_x[frame, :n_joints], sequence_y[frame, :n_joints], sequence_z[frame, :n_joints])\n",
    "        scatt2._offsets3d = (sequence_x[frame, n_joints:], sequence_y[frame, n_joints:], sequence_z[frame, n_joints:])\n",
    "    \n",
    "        if skeleton is not None:\n",
    "            for line, (start, end) in zip(lines, skeleton):\n",
    "                line.set_data([sequence_x[frame, start], sequence_x[frame, end]], [sequence_y[frame, start], sequence_y[frame, end]])\n",
    "                line.set_3d_properties([sequence_z[frame, start], sequence_z[frame, end]])\n",
    "            \n",
    "            return scatt1, scatt2, *lines\n",
    "\n",
    "        return scatt1, scatt2\n",
    "\n",
    "    plt.close(fig)\n",
    "    return FuncAnimation(fig, update, frames=range(len(sequence_x)), interval=interval, blit=False)\n",
    "\n",
    "# ani = animation(train_batches[sequence], skeletons, interval=100)\n",
    "# HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d48133e-93a9-491a-8d99-68c811c6d1c3",
   "metadata": {},
   "source": [
    "## NRI Graph Structure Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f880668-b607-4ad7-b669-a37038bdd49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading best model\n",
    "model.load_state_dict(torch.load('best_weights/nri_parameters.pt'))\n",
    "\n",
    "# Choosing sequence based on the selected video and frame, and preparing n_joints for plotting\n",
    "choreo_idx = 0\n",
    "offset = 0\n",
    "if choreo_idx != 0:\n",
    "    offset = np.cumsum(train_split)[choreo_idx-1]\n",
    "\n",
    "frame = 0\n",
    "sequence = frame // seq_len_in\n",
    "\n",
    "n_joints = int(batches.size(2)/2)\n",
    "\n",
    "# Getting predicted edges from a sequence\n",
    "logits = model.encoder(train_batches[offset + sequence].unsqueeze(0)).squeeze(0)\n",
    "edge_index_samp = np.array(edge_index_t)[torch.where(logits.argmax(dim=1))[0].cpu()]\n",
    "\n",
    "# Getting reconstruction for next sequence\n",
    "edge_index_samp_decoder = torch.Tensor(edge_index_t).to(device)[torch.where(logits.argmax(dim=1))[0]].t().long()\n",
    "decoder_m_in, decoder_m_out = message_passing_matrices(n_joints*2, edge_index_samp_decoder)\n",
    "decoder_m_in = decoder_m_in.to(device)\n",
    "decoder_m_out = decoder_m_out.to(device)\n",
    "\n",
    "recon_output = model.decoder(train_batches[offset + sequence].unsqueeze(0), \\\n",
    "                             edge_index_samp_decoder, decoder_m_in, decoder_m_out)\n",
    "recon_output = recon_output.view(seq_len_out, n_joints*2, dims).cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61ee5b3-6005-47c6-8c84-9b00b2a065f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting dancers\n",
    "fig = plt.figure(figsize=(12, 4))\n",
    "\n",
    "ax1 = fig.add_subplot(121, projection=\"3d\")\n",
    "ax1.set_xlim([-1, 1])\n",
    "ax1.set_ylim([-1, 1])\n",
    "ax1.set_zlim([-1, 1])\n",
    "ax1.set_xticks([])\n",
    "ax1.set_yticks([])\n",
    "ax1.set_zticks([])\n",
    "\n",
    "ax1.scatter(joint_poses[choreo_idx][frame, :n_joints, 2], joint_poses[choreo_idx][frame, :n_joints, 0], \\\n",
    "            -joint_poses[choreo_idx][frame, :n_joints, 1], color='red')\n",
    "ax1.scatter(joint_poses[choreo_idx][frame, n_joints:, 2], joint_poses[choreo_idx][frame, n_joints:, 0], \\\n",
    "            -joint_poses[choreo_idx][frame, n_joints:, 1], color='blue')\n",
    "\n",
    "for (start, end) in skeletons:\n",
    "    xs = [joint_poses[choreo_idx][frame, start, 2], joint_poses[choreo_idx][frame, end, 2]]\n",
    "    ys = [joint_poses[choreo_idx][frame, start, 0], joint_poses[choreo_idx][frame, end, 0]]\n",
    "    zs = [-joint_poses[choreo_idx][frame, start, 1], -joint_poses[choreo_idx][frame, end, 1]]\n",
    "    ax1.plot(xs, ys, zs, color='grey')\n",
    "\n",
    "# Plotting dancers with sampled edges\n",
    "ax2 = fig.add_subplot(122, projection=\"3d\")\n",
    "ax2.set_xlim([-1, 1])\n",
    "ax2.set_ylim([-1, 1])\n",
    "ax2.set_zlim([-1, 1])\n",
    "ax2.set_xticks([])\n",
    "ax2.set_yticks([])\n",
    "ax2.set_zticks([])\n",
    "\n",
    "ax2.scatter(joint_poses[choreo_idx][frame, :n_joints, 2], joint_poses[choreo_idx][frame, :n_joints, 0], \\\n",
    "            -joint_poses[choreo_idx][frame, :n_joints, 1], color='red')\n",
    "ax2.scatter(joint_poses[choreo_idx][frame, n_joints:, 2], joint_poses[choreo_idx][frame, n_joints:, 0], \\\n",
    "            -joint_poses[choreo_idx][frame, n_joints:, 1], color='blue')\n",
    "\n",
    "for (start, end) in edge_index_samp:\n",
    "    xs = [joint_poses[choreo_idx][frame, start, 2], joint_poses[choreo_idx][frame, end, 2]]\n",
    "    ys = [joint_poses[choreo_idx][frame, start, 0], joint_poses[choreo_idx][frame, end, 0]]\n",
    "    zs = [-joint_poses[choreo_idx][frame, start, 1], -joint_poses[choreo_idx][frame, end, 1]]\n",
    "    ax2.plot(xs, ys, zs, color='grey')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa502d9c-c36f-440c-8fb8-afd57aac0444",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video, display\n",
    "\n",
    "# Getting dataset back to CPU\n",
    "train_batches = train_batches.cpu()\n",
    "\n",
    "# Evaluating the edge prediction by watching the dance sequence with and without the connections\n",
    "ani = animation(train_batches[offset + sequence], skeletons, interval=100)\n",
    "ani_html_no_edge_pred = ani.to_jshtml()\n",
    "\n",
    "ani = animation(train_batches[offset + sequence], edge_index_samp, interval=100)\n",
    "ani_html_edge_pred = ani.to_jshtml()\n",
    "\n",
    "# Evaluating reconstruction by watching next dance sequence and the predicted one\n",
    "ani = animation(train_batches[offset + sequence + 1], skeletons, interval=100)\n",
    "ani_html_no_mov_pred = ani.to_jshtml()\n",
    "\n",
    "ani = animation(recon_output, skeletons, interval=100)\n",
    "ani_html_mov_pred = ani.to_jshtml()\n",
    "\n",
    "display(HTML(f\"\"\"\n",
    "<div style=\"display: flex; justify-content: space-around;\">\n",
    "    <div style=\"margin: 5px;\">\n",
    "        {ani_html_no_edge_pred}\n",
    "    </div>\n",
    "    <div style=\"margin: 5px;\">\n",
    "        {ani_html_edge_pred}\n",
    "    </div>\n",
    "</div>\n",
    "<div style=\"display: flex; justify-content: space-around;\">\n",
    "    <div style=\"margin: 5px;\">\n",
    "        {ani_html_no_mov_pred}\n",
    "    </div>\n",
    "    <div style=\"margin: 5px;\">\n",
    "        {ani_html_mov_pred}\n",
    "    </div>\n",
    "</div>\n",
    "\"\"\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
