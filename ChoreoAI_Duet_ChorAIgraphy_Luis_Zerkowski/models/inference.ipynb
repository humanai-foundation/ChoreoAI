{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f62a10e-1059-475f-8a24-bbd209449208",
   "metadata": {},
   "source": [
    "# Inference Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02b3f5a-2047-45d6-8698-b4ef52349757",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.core.debugger import set_trace\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "\n",
    "# Reading data coming from the pre-processing pipeline and creating both dancers\n",
    "joint_poses = []\n",
    "for file in glob('./data/*.npy'):\n",
    "    interleaved_poses = np.load(file)\n",
    "    poses_1 = interleaved_poses[0::2]\n",
    "    poses_2 = interleaved_poses[1::2]\n",
    "\n",
    "    # Sampling frames for movement smoothness\n",
    "    joint_poses.append(np.concatenate((poses_1, poses_2), axis=1)[::3])\n",
    "    print('Joint poses {} shape: {}\\n'.format(file.split('/')[-1], joint_poses[-1].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fb5544-15a2-445d-89cc-d4817d7ad5b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Building initial transposed edge index (adjacencies)\n",
    "edge_index_t = [[0, 1], [0, 2], [0, 3], [1, 4], [2, 5], [3, 6], [4, 7], [5, 8], [6, 9], [7, 10], [7, 27], [8, 11],\n",
    "                [8, 28], [9, 12], [9, 13], [9, 14], [12, 15], [13, 16], [14, 17], [15, 24], [16, 18], [17, 19],\n",
    "                [18, 20], [19, 21], [20, 22], [21, 23], [22, 25], [23, 26]]\n",
    "\n",
    "# Getting second person skeleton\n",
    "n_joints = int(joint_poses[0].shape[1]/2)\n",
    "init_skeleton_len = len(edge_index_t)\n",
    "for edge_index in range(init_skeleton_len):\n",
    "    edge_index_t.append([edge_index_t[edge_index][0]+n_joints, edge_index_t[edge_index][1]+n_joints])\n",
    "\n",
    "# Saving skeletons for visualization\n",
    "skeletons = edge_index_t.copy()\n",
    "\n",
    "# Fully connecting the two people\n",
    "for joint_1 in range(interleaved_poses.shape[1]):\n",
    "    for joint_2 in range(interleaved_poses.shape[1]):\n",
    "        edge_index_t.append([joint_1, joint_2+n_joints])\n",
    "\n",
    "# Making graph undirected\n",
    "full_skeleton_len = len(edge_index_t)\n",
    "for edge_index in range(full_skeleton_len):\n",
    "    edge_index_t.append([edge_index_t[edge_index][1], edge_index_t[edge_index][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6732d4-851a-4aa4-9ca6-3bbc68a52525",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from utils import *\n",
    "\n",
    "# Estimating velocity of points\n",
    "frame_gap = 1\n",
    "velocities = []\n",
    "joint_poses_velo = []\n",
    "for joint_pose in [joint_poses]:\n",
    "    velocities_aux = []\n",
    "    joint_poses_velo_aux = []\n",
    "    \n",
    "    for choreo in joint_pose:\n",
    "        choreo = torch.Tensor(choreo)\n",
    "    \n",
    "        velocity_choreo = compute_velocities(choreo, frame_gap)\n",
    "        velocities_aux.append(velocity_choreo)\n",
    "        \n",
    "        joint_poses_velo_aux.append(torch.cat((choreo, velocity_choreo), dim=-1))\n",
    "\n",
    "    velocities.append(velocities_aux)\n",
    "    joint_poses_velo.append(joint_poses_velo_aux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86400886-0ec0-4b9a-bc2c-67cfc8c1ddbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = 'param_grid_n-sample-dancers_5_dataset-multiplier_10_compact-encoder_False_recurrent-encoder_False_seq-len-in_8_seq-len-out_1_hidden-dim_64_edge-types_3_epochs_20_lr_0.0003_lr-cycle_5_loss-mode_mse'\n",
    "\n",
    "seq_len = int(path.split('seq-len-in_')[1].split('_')[0])\n",
    "batches = []\n",
    "choreo_lens = []\n",
    "\n",
    "# # Building non-overlapping sequences\n",
    "# for joint_poses_rot_velo in joint_poses_velo:\n",
    "#     for choreo in joint_poses_rot_velo:\n",
    "#         choreo = torch.Tensor(choreo)\n",
    "    \n",
    "#         num_seqs = choreo.shape[0] // seq_len\n",
    "#         batches.append(torch.stack([choreo[i*seq_len:(i+1)*seq_len] for i in range(num_seqs)]))\n",
    "#         choreo_lens.append(batches[-1].size(0))\n",
    "\n",
    "# Building overlapping sequences\n",
    "for joint_poses_rot_velo in joint_poses_velo:\n",
    "    for choreo in joint_poses_rot_velo:\n",
    "        choreo = torch.Tensor(choreo)\n",
    "    \n",
    "        batches.append(torch.stack([choreo[i:i+seq_len] for i in range(len(choreo)-seq_len)]))\n",
    "        choreo_lens.append(batches[-1].size(0))\n",
    "\n",
    "batches = torch.cat(batches, dim=0)\n",
    "\n",
    "# Fixing x, y, z configuration\n",
    "batches[:, :, :, [0, 1, 2]] = batches[:, :, :, [2, 0, 1]]\n",
    "batches[:, :, :, 2] = -batches[:, :, :, 2]\n",
    "\n",
    "# Balanced training-validation split\n",
    "train_split = []\n",
    "val_split = []\n",
    "train_batches = []\n",
    "val_batches = []\n",
    "next_choreo = 0\n",
    "for choreo_len in choreo_lens:\n",
    "    \n",
    "    train_split.append(int(0.85*choreo_len))\n",
    "    val_split.append(choreo_len - train_split[-1])\n",
    "    \n",
    "    train_batches.append(batches[next_choreo : next_choreo + train_split[-1]])\n",
    "    val_batches.append(batches[next_choreo + train_split[-1] : next_choreo + choreo_len])\n",
    "    \n",
    "    next_choreo += choreo_len\n",
    "\n",
    "train_batches = torch.cat(train_batches, dim=0)\n",
    "val_batches = torch.cat(val_batches, dim=0)\n",
    "\n",
    "# Printing all the data structures created\n",
    "print('Shape of tensor with all sequences: {}'.format(batches.shape))\n",
    "print('Length of each choreography: {}\\n'.format(choreo_lens))\n",
    "\n",
    "print('Shape of training data with all sequences: {}'.format(train_batches.shape))\n",
    "print('Length of each choreography in training dataset: {}\\n'.format(train_split))\n",
    "\n",
    "print('Shape of validation data with all sequences: {}'.format(val_batches.shape))\n",
    "print('Length of each choreography in validation dataset: {}\\n'.format(val_split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc23e0d9-4dd6-462c-8f0a-b6ab8a2806be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "# Sampling dancers joints to simplify problem\n",
    "output_path = 'outputs/' + path + '.out'\n",
    "output_file = open(output_path, 'r')\n",
    "dancer_joints = output_file.read().split('dancer 1: ')[1]\n",
    "\n",
    "first_list = dancer_joints.split(',')[0].replace(' ', ',')\n",
    "if ',,' in first_list:\n",
    "    first_list = first_list.replace(',,', ',')\n",
    "if '[,' in first_list:\n",
    "    first_list = first_list.replace('[,', '[')\n",
    "\n",
    "second_list = dancer_joints.split('dancer 2: ')[1].split('\\n')[0].replace(' ', ',')\n",
    "if ',,' in first_list:\n",
    "    second_list = second_list.replace(',,', ',')\n",
    "if '[,' in second_list:\n",
    "    second_list = second_list.replace('[,', '[')\n",
    "\n",
    "sampled_joints_1 = ast.literal_eval(first_list)\n",
    "sampled_joints_2 = ast.literal_eval(second_list)\n",
    "\n",
    "sampled_joints = np.concatenate([sampled_joints_1, sampled_joints_2])\n",
    "n_joints_sampled = len(sampled_joints)\n",
    "print(\"Sampled joints for dancer 1: {}, and dancer 2: {}\".format(sampled_joints_1, sampled_joints_2))\n",
    "\n",
    "# Mapping sampled joints to new indices for the message passing matrices\n",
    "edge_mapping = {e: c for c, e in zip(range(len(sampled_joints)), sampled_joints)}\n",
    "inverse_edge_mapping = {c: e for c, e in zip(range(len(sampled_joints)), sampled_joints)}\n",
    "\n",
    "sampled_batches = batches[:, :, sampled_joints, :]\n",
    "sampled_train_batches = train_batches[:, :, sampled_joints, :]\n",
    "sampled_val_batches = val_batches[:, :, sampled_joints, :]\n",
    "\n",
    "print('Shape of sampled data with all sequences: {}'.format(sampled_batches.shape))\n",
    "print('Shape of sampled training data with all sequences: {}'.format(sampled_train_batches.shape))\n",
    "print('Shape of sampled validation data with all sequences: {}'.format(sampled_val_batches.shape))\n",
    "\n",
    "sampled_edge_index_t = []\n",
    "for (start, end) in edge_index_t[:len(edge_index_t)]:\n",
    "    if (start not in sampled_joints) or (end not in sampled_joints):\n",
    "        continue\n",
    "\n",
    "    sampled_edge_index_t.append([edge_mapping[start], edge_mapping[end]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b73c757-62e0-4ab0-abbd-c5f3379a605b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from modules import *\n",
    "\n",
    "# Defining standard parameters\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "sampled_train_batches = sampled_train_batches.to(device)\n",
    "sampled_val_batches = sampled_val_batches.to(device)\n",
    "batch_size = seq_len\n",
    "\n",
    "train_batches_cumsum = np.cumsum(np.array(train_split) // batch_size)\n",
    "val_batches_cumsum = np.cumsum(np.array(val_split) // batch_size)\n",
    "\n",
    "weights_path = 'best_weights/nri_parameters_' + path + '.pt'\n",
    "\n",
    "seq_len_in = seq_len\n",
    "seq_len_out = 1\n",
    "n_joints = sampled_batches.size(2)\n",
    "dims = batches.size(3)\n",
    "\n",
    "hidden_dim = int(weights_path.split('hidden-dim_')[1].split('_')[0])\n",
    "edge_types = int(weights_path.split('edge-types_')[1].split('_')[0])\n",
    "tau = 0.5\n",
    "hard = True\n",
    "dropout = 0.1\n",
    "out_var = 5e-5\n",
    "\n",
    "if edge_types == 3:\n",
    "    prior = [0.4, 0.3, 0.3]\n",
    "else:\n",
    "    prior = [0.35, 0.25, 0.15, 0.25]\n",
    "log_prior = torch.FloatTensor(np.log(prior)).unsqueeze(0).unsqueeze(0).to(device)\n",
    "\n",
    "compact_encoder = True if weights_path.split('compact-encoder_')[1].split('_')[0] == 'True' else False\n",
    "recurrent_encoder = True if weights_path.split('recurrent-encoder_')[1].split('_')[0] == 'True' else False\n",
    "recurrent_decoder = True\n",
    "\n",
    "# Loading model\n",
    "model = nri_vae(device, n_joints, sampled_edge_index_t, seq_len_in*dims, hidden_dim, edge_types, seq_len_out*int(dims/2), \\\n",
    "                tau, hard, dropout, dims, compact_encoder, recurrent_encoder, recurrent_decoder)\n",
    "\n",
    "model.load_state_dict(torch.load(weights_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd9ae82-d034-4232-b36e-26e210975016",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Choosing sequence based on the selected video and frame, and preparing n_joints for plotting\n",
    "choreo_idx = np.random.randint(len(joint_poses))\n",
    "# choreo_idx = 1\n",
    "offset = 0\n",
    "if choreo_idx != 0:\n",
    "    offset = np.cumsum(train_split)[choreo_idx-1]\n",
    "    # offset = np.cumsum(val_split)[choreo_idx-1]\n",
    "\n",
    "frame = np.random.randint(joint_poses[choreo_idx].shape[0])\n",
    "# frame = np.random.randint(sampled_val_batches.shape[0])\n",
    "# frame = 50\n",
    "sequence = frame // seq_len_in\n",
    "\n",
    "n_joints = int(sampled_batches.size(2)/2)\n",
    "\n",
    "# Getting predicted edges from a sequence\n",
    "logits = model.encoder(sampled_train_batches[offset + sequence].unsqueeze(0)).squeeze(0)\n",
    "# logits = model.encoder(sampled_val_batches[offset + sequence].unsqueeze(0)).squeeze(0)\n",
    "\n",
    "# Confidence threshold\n",
    "confidence = 0.8\n",
    "chosen_edge_type = -1\n",
    "edge_index_classes = []\n",
    "while len(edge_index_classes) <= 2:\n",
    "    y = gumbel_softmax_sample(logits, tau, False)\n",
    "    edge_index_classes = torch.where(y[:, chosen_edge_type] > confidence)[0]\n",
    "\n",
    "# # Top % edges\n",
    "# percentage = 0.01\n",
    "# chosen_edge_type = -1\n",
    "# k = int(len(edge_index_t)*percentage)\n",
    "# edge_index_classes = torch.topk(y[:, chosen_edge_type], k)[1]\n",
    "\n",
    "edge_index_samp = torch.Tensor(sampled_edge_index_t).to(device)[edge_index_classes].t().long()\n",
    "print('Amount of edges sampled with confidence {}%: {}'.format(int(confidence*100), len(edge_index_classes)))\n",
    "# print('Top {}% edges: {}'.format(int(percentage*100), len(edge_index_classes)))\n",
    "\n",
    "# Getting reconstructed frame\n",
    "decoder_m_in, decoder_m_out = message_passing_matrices(n_joints*2, edge_index_samp)\n",
    "decoder_m_in = decoder_m_in.to(device)\n",
    "decoder_m_out = decoder_m_out.to(device)\n",
    "\n",
    "recon_list = []\n",
    "for i in range(4*seq_len_in):\n",
    "    recon_output = model.decoder(sampled_train_batches[offset + sequence], \\\n",
    "                                     edge_index_samp, decoder_m_in, decoder_m_out)\n",
    "    recon_output = recon_output.view(seq_len_out, n_joints*2, int(dims/2)).cpu().detach().numpy()\n",
    "\n",
    "    if seq_len_out == 1:\n",
    "        recon_output = recon_output.squeeze(0)\n",
    "        \n",
    "    recon_list.append(recon_output)\n",
    "    \n",
    "edge_index_samp = torch.Tensor([[inverse_edge_mapping[e[0].item()], inverse_edge_mapping[e[1].item()]] for e in edge_index_samp.t().cpu().detach()]).to(device).t().long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484f7a0f-5a62-48d3-af8f-999626825b9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Plotting reconstructed frame\n",
    "ax1 = fig.add_subplot(131, projection=\"3d\")\n",
    "ax1.set_title('Reconstructed Frame')\n",
    "ax1.set_xlim([-1, 1])\n",
    "ax1.set_ylim([-1, 1])\n",
    "ax1.set_zlim([-1, 1])\n",
    "ax1.set_xticks([])\n",
    "ax1.set_yticks([])\n",
    "ax1.set_zticks([])\n",
    "\n",
    "ax1.scatter(recon_output[:n_joints, 0], recon_output[:n_joints, 1], recon_output[:n_joints, 2], color='orange', s=60, alpha=0.9)\n",
    "ax1.scatter(recon_output[n_joints:, 0], recon_output[n_joints:, 1], recon_output[n_joints:, 2], color='purple', s=60, alpha=0.9)\n",
    "\n",
    "dancers_no_rec = np.array(list(set(np.arange(joint_poses[0].shape[1])) - set(sampled_joints)))\n",
    "ax1.scatter(joint_poses[choreo_idx][frame, dancers_no_rec[:int(len(dancers_no_rec)/2)], 2], joint_poses[choreo_idx][frame, dancers_no_rec[:int(len(dancers_no_rec)/2)], 0], \\\n",
    "            -joint_poses[choreo_idx][frame, dancers_no_rec[:int(len(dancers_no_rec)/2)], 1], color='red')\n",
    "ax1.scatter(joint_poses[choreo_idx][frame, dancers_no_rec[int(len(dancers_no_rec)/2):], 2], joint_poses[choreo_idx][frame, dancers_no_rec[int(len(dancers_no_rec)/2):], 0], \\\n",
    "            -joint_poses[choreo_idx][frame, dancers_no_rec[int(len(dancers_no_rec)/2):], 1], color='blue')\n",
    "\n",
    "for (start, end) in skeletons:\n",
    "    if start in sampled_joints:\n",
    "        xs_0 = recon_output[edge_mapping[start], 0]\n",
    "        ys_0 = recon_output[edge_mapping[start], 1]\n",
    "        zs_0 = recon_output[edge_mapping[start], 2]\n",
    "    else:\n",
    "        xs_0 = joint_poses[choreo_idx][frame, start, 2]\n",
    "        ys_0 = joint_poses[choreo_idx][frame, start, 0]\n",
    "        zs_0 = -joint_poses[choreo_idx][frame, start, 1]\n",
    "    \n",
    "    if end in sampled_joints:\n",
    "        xs_1 = recon_output[edge_mapping[end], 0]\n",
    "        ys_1 = recon_output[edge_mapping[end], 1]\n",
    "        zs_1 = recon_output[edge_mapping[end], 2]\n",
    "    else:\n",
    "        xs_1 = joint_poses[choreo_idx][frame, end, 2]\n",
    "        ys_1 = joint_poses[choreo_idx][frame, end, 0]\n",
    "        zs_1 = -joint_poses[choreo_idx][frame, end, 1]\n",
    "    \n",
    "    xs = [xs_0, xs_1]\n",
    "    ys = [ys_0, ys_1]\n",
    "    zs = [zs_0, zs_1]\n",
    "    ax1.plot(xs, ys, zs, color='grey')\n",
    "\n",
    "# Plotting sampled edges\n",
    "ax2 = fig.add_subplot(132, projection=\"3d\")\n",
    "ax2.set_title('Sampled Edges')\n",
    "ax2.set_xlim([-1, 1])\n",
    "ax2.set_ylim([-1, 1])\n",
    "ax2.set_zlim([-1, 1])\n",
    "ax2.set_xticks([])\n",
    "ax2.set_yticks([])\n",
    "ax2.set_zticks([])\n",
    "\n",
    "ax2.scatter(joint_poses[choreo_idx][frame, :int(joint_poses[0].shape[1]/2), 2], joint_poses[choreo_idx][frame, :int(joint_poses[0].shape[1]/2), 0], \\\n",
    "            -joint_poses[choreo_idx][frame, :int(joint_poses[0].shape[1]/2), 1], color='red')\n",
    "ax2.scatter(joint_poses[choreo_idx][frame, int(joint_poses[0].shape[1]/2):, 2], joint_poses[choreo_idx][frame, int(joint_poses[0].shape[1]/2):, 0], \\\n",
    "            -joint_poses[choreo_idx][frame, int(joint_poses[0].shape[1]/2):, 1], color='blue')\n",
    "\n",
    "for (start, end) in skeletons:\n",
    "    xs = [joint_poses[choreo_idx][frame, start, 2], joint_poses[choreo_idx][frame, end, 2]]\n",
    "    ys = [joint_poses[choreo_idx][frame, start, 0], joint_poses[choreo_idx][frame, end, 0]]\n",
    "    zs = [-joint_poses[choreo_idx][frame, start, 1], -joint_poses[choreo_idx][frame, end, 1]]\n",
    "    ax2.plot(xs, ys, zs, color='grey')\n",
    "\n",
    "y_min = torch.min(y[edge_index_classes, chosen_edge_type])\n",
    "y_max = torch.max(y[edge_index_classes, chosen_edge_type])\n",
    "alphas = 0.4 + 0.6 * (y[edge_index_classes, chosen_edge_type] - y_min)/(y_max - y_min)\n",
    "for i, a in enumerate(alphas):\n",
    "    if a > 1:\n",
    "        alphas[i] = 1.\n",
    "for i, (start, end) in enumerate(edge_index_samp.t()):\n",
    "    alpha = alphas[i].cpu().detach().item()\n",
    "    \n",
    "    xs = [joint_poses[choreo_idx][frame, start, 2], joint_poses[choreo_idx][frame, end, 2]]\n",
    "    ys = [joint_poses[choreo_idx][frame, start, 0], joint_poses[choreo_idx][frame, end, 0]]\n",
    "    zs = [-joint_poses[choreo_idx][frame, start, 1], -joint_poses[choreo_idx][frame, end, 1]]\n",
    "    \n",
    "    ax2.plot(xs, ys, zs, color='black', alpha=alpha)\n",
    "    \n",
    "# Plotting directed sampled edges\n",
    "ax3 = fig.add_subplot(133, projection=\"3d\")\n",
    "ax3.set_title('Directed Sampled Edges')\n",
    "ax3.set_xlim([-1, 1])\n",
    "ax3.set_ylim([-1, 1])\n",
    "ax3.set_zlim([-1, 1])\n",
    "ax3.set_xticks([])\n",
    "ax3.set_yticks([])\n",
    "ax3.set_zticks([])\n",
    "\n",
    "ax3.scatter(joint_poses[choreo_idx][frame, :int(joint_poses[0].shape[1]/2), 2], joint_poses[choreo_idx][frame, :int(joint_poses[0].shape[1]/2), 0], \\\n",
    "            -joint_poses[choreo_idx][frame, :int(joint_poses[0].shape[1]/2), 1], color='red')\n",
    "ax3.scatter(joint_poses[choreo_idx][frame, int(joint_poses[0].shape[1]/2):, 2], joint_poses[choreo_idx][frame, int(joint_poses[0].shape[1]/2):, 0], \\\n",
    "            -joint_poses[choreo_idx][frame, int(joint_poses[0].shape[1]/2):, 1], color='blue')\n",
    "\n",
    "for (start, end) in skeletons:\n",
    "    xs = [joint_poses[choreo_idx][frame, start, 2], joint_poses[choreo_idx][frame, end, 2]]\n",
    "    ys = [joint_poses[choreo_idx][frame, start, 0], joint_poses[choreo_idx][frame, end, 0]]\n",
    "    zs = [-joint_poses[choreo_idx][frame, start, 1], -joint_poses[choreo_idx][frame, end, 1]]\n",
    "    ax3.plot(xs, ys, zs, color='grey')\n",
    "\n",
    "for i, (start, end) in enumerate(edge_index_samp.t()):\n",
    "    alpha = alphas[i].cpu().detach().item()\n",
    "        \n",
    "    direction_vector = joint_poses[choreo_idx][frame, end] - joint_poses[choreo_idx][frame, start]\n",
    "    \n",
    "    ax3.quiver(joint_poses[choreo_idx][frame, start, 2], joint_poses[choreo_idx][frame, start, 0], -joint_poses[choreo_idx][frame, start, 1], \\\n",
    "               direction_vector[2], direction_vector[0], -direction_vector[1], color='black', alpha=alpha, arrow_length_ratio=0.2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca54069-7725-4709-b551-04cb7678b2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML, Video, display\n",
    "\n",
    "# Define the update function for animation\n",
    "def update(ani_frame):\n",
    "    ax1.cla()\n",
    "    \n",
    "    # Original sequence\n",
    "    ax1.set_title('Original Sequence')\n",
    "    ax1.set_xlim([-1, 1])\n",
    "    ax1.set_ylim([-1, 1])\n",
    "    ax1.set_zlim([-1, 1])\n",
    "    ax1.set_xticks([])\n",
    "    ax1.set_yticks([])\n",
    "    ax1.set_zticks([])\n",
    "    \n",
    "    recon_output = recon_list[ani_frame]\n",
    "    \n",
    "    ax1.scatter(recon_output[:n_joints, 0], recon_output[:n_joints, 1], recon_output[:n_joints, 2], color='orange', s=60, alpha=0.9)\n",
    "    ax1.scatter(recon_output[n_joints:, 0], recon_output[n_joints:, 1], recon_output[n_joints:, 2], color='purple', s=60, alpha=0.9)\n",
    "    \n",
    "    dancers_no_rec = np.array(list(set(np.arange(joint_poses[0].shape[1])) - set(sampled_joints)))\n",
    "    ax1.scatter(joint_poses[choreo_idx][frame+ani_frame, dancers_no_rec[:int(len(dancers_no_rec)/2)], 2], \n",
    "                joint_poses[choreo_idx][frame+ani_frame, dancers_no_rec[:int(len(dancers_no_rec)/2)], 0], \n",
    "                -joint_poses[choreo_idx][frame+ani_frame, dancers_no_rec[:int(len(dancers_no_rec)/2)], 1], color='red')\n",
    "    ax1.scatter(joint_poses[choreo_idx][frame+ani_frame, dancers_no_rec[int(len(dancers_no_rec)/2):], 2], \n",
    "                joint_poses[choreo_idx][frame+ani_frame, dancers_no_rec[int(len(dancers_no_rec)/2):], 0], \n",
    "                -joint_poses[choreo_idx][frame+ani_frame, dancers_no_rec[int(len(dancers_no_rec)/2):], 1], color='blue')\n",
    "    \n",
    "    for (start, end) in skeletons:\n",
    "        if start in sampled_joints:\n",
    "            xs_0 = recon_output[edge_mapping[start], 0]\n",
    "            ys_0 = recon_output[edge_mapping[start], 1]\n",
    "            zs_0 = recon_output[edge_mapping[start], 2]\n",
    "        else:\n",
    "            xs_0 = joint_poses[choreo_idx][frame+ani_frame, start, 2]\n",
    "            ys_0 = joint_poses[choreo_idx][frame+ani_frame, start, 0]\n",
    "            zs_0 = -joint_poses[choreo_idx][frame+ani_frame, start, 1]\n",
    "        \n",
    "        if end in sampled_joints:\n",
    "            xs_1 = recon_output[edge_mapping[end], 0]\n",
    "            ys_1 = recon_output[edge_mapping[end], 1]\n",
    "            zs_1 = recon_output[edge_mapping[end], 2]\n",
    "        else:\n",
    "            xs_1 = joint_poses[choreo_idx][frame+ani_frame, end, 2]\n",
    "            ys_1 = joint_poses[choreo_idx][frame+ani_frame, end, 0]\n",
    "            zs_1 = -joint_poses[choreo_idx][frame+ani_frame, end, 1]\n",
    "        \n",
    "        xs = [xs_0, xs_1]\n",
    "        ys = [ys_0, ys_1]\n",
    "        zs = [zs_0, zs_1]\n",
    "        ax1.plot(xs, ys, zs, color='grey')\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax1 = fig.add_subplot(111, projection=\"3d\")\n",
    "\n",
    "# Create animation\n",
    "ani = FuncAnimation(fig, update, frames=len(recon_list), repeat=True, interval=100)\n",
    "ani = ani.to_jshtml()\n",
    "\n",
    "display(HTML(f\"\"\"\n",
    "<div style=\"display: flex; justify-content: space-around;\">\n",
    "    <div style=\"margin: 5px;\">\n",
    "        {ani}\n",
    "    </div>\n",
    "</div>\n",
    "\"\"\"))\n",
    "\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7299143d-3e29-4aad-b023-24776e97e734",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML, Video, display\n",
    "\n",
    "# Define the update function for animation\n",
    "def update(ani_frame):\n",
    "    ax1.cla()\n",
    "    \n",
    "    # Sampled edges\n",
    "    ax1.set_title('Sampled Edges')\n",
    "    ax1.set_xlim([-1, 1])\n",
    "    ax1.set_ylim([-1, 1])\n",
    "    ax1.set_zlim([-1, 1])\n",
    "    ax1.set_xticks([])\n",
    "    ax1.set_yticks([])\n",
    "    ax1.set_zticks([])\n",
    "\n",
    "    ax1.scatter(joint_poses[choreo_idx][frame+ani_frame, :int(joint_poses[0].shape[1]/2), 2], \n",
    "                joint_poses[choreo_idx][frame+ani_frame, :int(joint_poses[0].shape[1]/2), 0], \n",
    "                -joint_poses[choreo_idx][frame+ani_frame, :int(joint_poses[0].shape[1]/2), 1], color='red')\n",
    "    ax1.scatter(joint_poses[choreo_idx][frame+ani_frame, int(joint_poses[0].shape[1]/2):, 2], \n",
    "                joint_poses[choreo_idx][frame+ani_frame, int(joint_poses[0].shape[1]/2):, 0], \n",
    "                -joint_poses[choreo_idx][frame+ani_frame, int(joint_poses[0].shape[1]/2):, 1], color='blue')\n",
    "\n",
    "    for (start, end) in skeletons:\n",
    "        xs = [joint_poses[choreo_idx][frame+ani_frame, start, 2], joint_poses[choreo_idx][frame+ani_frame, end, 2]]\n",
    "        ys = [joint_poses[choreo_idx][frame+ani_frame, start, 0], joint_poses[choreo_idx][frame+ani_frame, end, 0]]\n",
    "        zs = [-joint_poses[choreo_idx][frame+ani_frame, start, 1], -joint_poses[choreo_idx][frame+ani_frame, end, 1]]\n",
    "        ax1.plot(xs, ys, zs, color='grey')\n",
    "\n",
    "    for i, (start, end) in enumerate(edge_index_samp.t()):\n",
    "        alpha = alphas[i].cpu().detach().item()\n",
    "        \n",
    "        xs = [joint_poses[choreo_idx][frame+ani_frame, start, 2], joint_poses[choreo_idx][frame+ani_frame, end, 2]]\n",
    "        ys = [joint_poses[choreo_idx][frame+ani_frame, start, 0], joint_poses[choreo_idx][frame+ani_frame, end, 0]]\n",
    "        zs = [-joint_poses[choreo_idx][frame+ani_frame, start, 1], -joint_poses[choreo_idx][frame+ani_frame, end, 1]]\n",
    "        \n",
    "        ax1.plot(xs, ys, zs, color='black', alpha=alpha)\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax1 = fig.add_subplot(111, projection=\"3d\")\n",
    "\n",
    "# Create animation\n",
    "ani = FuncAnimation(fig, update, frames=len(recon_list), repeat=True, interval=100)\n",
    "ani = ani.to_jshtml()\n",
    "\n",
    "display(HTML(f\"\"\"\n",
    "<div style=\"display: flex; justify-content: space-around;\">\n",
    "    <div style=\"margin: 5px;\">\n",
    "        {ani}\n",
    "    </div>\n",
    "</div>\n",
    "\"\"\"))\n",
    "\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2c41e2-3071-483b-8292-3a92d8387fe6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML, Video, display\n",
    "\n",
    "# Define the update function for animation\n",
    "def update(ani_frame):\n",
    "    ax1.cla()\n",
    "    \n",
    "    # Directed sampled edges\n",
    "    ax1.set_title('Directed Sampled Edges')\n",
    "    ax1.set_xlim([-1, 1])\n",
    "    ax1.set_ylim([-1, 1])\n",
    "    ax1.set_zlim([-1, 1])\n",
    "    ax1.set_xticks([])\n",
    "    ax1.set_yticks([])\n",
    "    ax1.set_zticks([])\n",
    "    \n",
    "    ax1.scatter(joint_poses[choreo_idx][frame+ani_frame, :int(joint_poses[0].shape[1]/2), 2], joint_poses[choreo_idx][frame+ani_frame, :int(joint_poses[0].shape[1]/2), 0], \\\n",
    "                -joint_poses[choreo_idx][frame+ani_frame, :int(joint_poses[0].shape[1]/2), 1], color='red')\n",
    "    ax1.scatter(joint_poses[choreo_idx][frame+ani_frame, int(joint_poses[0].shape[1]/2):, 2], joint_poses[choreo_idx][frame+ani_frame, int(joint_poses[0].shape[1]/2):, 0], \\\n",
    "                -joint_poses[choreo_idx][frame+ani_frame, int(joint_poses[0].shape[1]/2):, 1], color='blue')\n",
    "    \n",
    "    for (start, end) in skeletons:\n",
    "        xs = [joint_poses[choreo_idx][frame+ani_frame, start, 2], joint_poses[choreo_idx][frame+ani_frame, end, 2]]\n",
    "        ys = [joint_poses[choreo_idx][frame+ani_frame, start, 0], joint_poses[choreo_idx][frame+ani_frame, end, 0]]\n",
    "        zs = [-joint_poses[choreo_idx][frame+ani_frame, start, 1], -joint_poses[choreo_idx][frame+ani_frame, end, 1]]\n",
    "        ax1.plot(xs, ys, zs, color='grey')\n",
    "    \n",
    "    for i, (start, end) in enumerate(edge_index_samp.t()):\n",
    "        alpha = alphas[i].cpu().detach().item()\n",
    "            \n",
    "        direction_vector = joint_poses[choreo_idx][frame+ani_frame, end] - joint_poses[choreo_idx][frame+ani_frame, start]\n",
    "        \n",
    "        ax1.quiver(joint_poses[choreo_idx][frame+ani_frame, start, 2], joint_poses[choreo_idx][frame+ani_frame, start, 0], -joint_poses[choreo_idx][frame+ani_frame, start, 1], \\\n",
    "                   direction_vector[2], direction_vector[0], -direction_vector[1], color='black', alpha=alpha, arrow_length_ratio=0.2)\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax1 = fig.add_subplot(111, projection=\"3d\")\n",
    "\n",
    "# Create animation\n",
    "ani = FuncAnimation(fig, update, frames=len(recon_list), repeat=True, interval=100)\n",
    "ani = ani.to_jshtml()\n",
    "\n",
    "display(HTML(f\"\"\"\n",
    "<div style=\"display: flex; justify-content: space-around;\">\n",
    "    <div style=\"margin: 5px;\">\n",
    "        {ani}\n",
    "    </div>\n",
    "</div>\n",
    "\"\"\"))\n",
    "\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
